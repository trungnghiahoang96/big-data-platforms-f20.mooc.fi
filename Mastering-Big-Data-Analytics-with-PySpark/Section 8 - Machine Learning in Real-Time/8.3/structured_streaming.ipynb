{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/13 10:46:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 30\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "# SETTINGS\n",
    "IN_PATH = \"/home/nghiaht7/data-engineer/big-data-platforms-f20.mooc.fi/Mastering-Big-Data-Analytics-with-PySpark/Section 8 - Machine Learning in Real-Time/twitter_data/twitter/\"\n",
    "OUT_PATH = \"\"\n",
    "timestampformat = \"EEE MMM dd HH:mm:ss zzzz yyyy\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StructuredStreamingExample\").getOrCreate()\n",
    "schema = spark.read.json(IN_PATH).limit(10).schema\n",
    "\n",
    "# regular spark reader\n",
    "static_spark_reader = spark.read.schema(schema)\n",
    "\n",
    "# streaming spark reader\n",
    "stream_spark_reader = spark.readStream.schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle between spark streaming and batch mode by changing the spark_reader below\n",
    "# spark_reader = static_spark_reader\n",
    "spark_reader = stream_spark_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are streaming!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/13 14:04:56 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7c257f66-38e4-4df1-94b4-060ad52eb892. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark_reader.json(IN_PATH)\n",
    "    .select(\n",
    "        \"id\",\n",
    "        # extract proper timestamp from created_at column\n",
    "        f.to_timestamp(f.col(\"created_at\"), timestampformat).alias(\"timestamp\"),\n",
    "        # extract user information\n",
    "        f.col(\"user.screen_name\").alias(\"user\"),\n",
    "        \"text\",\n",
    "    )\n",
    "    .coalesce(1)\n",
    ")\n",
    "\n",
    "distinct_user_count = df.select(f.approx_count_distinct(\"user\"), f.current_timestamp())\n",
    "\n",
    "if not df.isStreaming:\n",
    "    print(\"Plain old, basic DataFrame - meh!\")\n",
    "    # Some actions only work on non-streaming DataFrames, like show and toPandas\n",
    "    distinct_user_count.show()\n",
    "    display(df.limit(25).toPandas())\n",
    "else:\n",
    "    print(\"We are streaming!\")\n",
    "    # Creating a DataSreamWriter and StreamingQuery\n",
    "    # ===\n",
    "    # Calling .writeStream on a DataFrame returns an instance of DataStreamWriter\n",
    "    stream_writer = (\n",
    "        distinct_user_count.writeStream\n",
    "        # DataStream queries need to be named\n",
    "        .queryName(\"distinct_user_count\")\n",
    "        .trigger(\n",
    "            # processingTime=\"5 seconds\",\n",
    "            # Setting 'once' to True will make spark only process the stream 1 time - great for debugging\n",
    "            once=True,  \n",
    "        )\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"memory\")\n",
    "    )\n",
    "    # Calling .start on a DataStreamWriter return an instance of StreamingQuery\n",
    "    query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .isStreaming can be used to determine if DataFrame is of Streaming kind or not\n",
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .isActive shows if the query is actively running or not\n",
    "query.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/13 13:28:16 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d00893ca-e4b7-423a-947a-f433d994aa3a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "# .start() transforms a DataStreamWriter to a StreamingQuery and starts the query execution\n",
    "if not query.isActive:\n",
    "    query = stream_writer.start()\n",
    "    \n",
    "# Calling .start on an already active StreamingQuery will raise an IllegalArgumentException\n",
    "# -> 'Cannot start query with name {StreamingQuery.name} as a query with that name is already active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .stop() stops the query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9b20d0f7-274d-4160-b4ec-6a1825e92d4f',\n",
       " 'runId': '5b66efe3-7b0c-4794-bee7-884a971e9022',\n",
       " 'name': 'distinct_user_count',\n",
       " 'timestamp': '2021-10-13T04:26:57.759Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 50,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 28.312570781426952,\n",
       " 'durationMs': {'addBatch': 740,\n",
       "  'getBatch': 717,\n",
       "  'latestOffset': 165,\n",
       "  'queryPlanning': 55,\n",
       "  'triggerExecution': 1766,\n",
       "  'walCommit': 19},\n",
       " 'stateOperators': [{'numRowsTotal': 1,\n",
       "   'numRowsUpdated': 1,\n",
       "   'memoryUsedBytes': 856,\n",
       "   'numRowsDroppedByWatermark': 0,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 0,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 712}}],\n",
       " 'sources': [{'description': 'FileStreamSource[file:/home/nghiaht7/data-engineer/big-data-platforms-f20.mooc.fi/Mastering-Big-Data-Analytics-with-PySpark/Section 8 - Machine Learning in Real-Time/twitter_data/twitter]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': {'logOffset': 0},\n",
       "   'numInputRows': 50,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 28.312570781426952}],\n",
       " 'sink': {'description': 'MemorySink', 'numOutputRows': 1}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .lastProgress shows information on the last processed batch\n",
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approx_count_distinct(user)</th>\n",
       "      <th>current_timestamp()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>2021-10-13 11:26:57.924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   approx_count_distinct(user)     current_timestamp()\n",
       "0                           49 2021-10-13 11:26:57.924"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.sql can be used to request how the query is performing\n",
    "display(spark.sql(f\"SELECT * from {query.name}\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live view ended...\n"
     ]
    }
   ],
   "source": [
    "# show live results for 2 minutes, refreshed every 1 second\n",
    "from time import sleep\n",
    "for x in range(0, 120):\n",
    "    # spark.sql can be used to request how the query is performing\n",
    "    display(spark.sql(f\"SELECT * from {query.name}\").toPandas())\n",
    "    sleep(1)\n",
    "    clear_output(wait=True)\n",
    "else:\n",
    "    print(\"Live view ended...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nFileSource[/home/nghiaht7/data-engineer/big-data-platforms-f20.mooc.fi/Mastering-Big-Data-Analytics-with-PySpark/Section 8 - Machine Learning in Real-Time/twitter_data/twitter/]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31341/755456499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# .show() will throw an error on Queries and Streaming DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdistinct_user_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/data-engineer/.venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data-engineer/.venv/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data-engineer/.venv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\nFileSource[/home/nghiaht7/data-engineer/big-data-platforms-f20.mooc.fi/Mastering-Big-Data-Analytics-with-PySpark/Section 8 - Machine Learning in Real-Time/twitter_data/twitter/]"
     ]
    }
   ],
   "source": [
    "# .show() will throw an error on Queries and Streaming DataFrames\n",
    "distinct_user_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
