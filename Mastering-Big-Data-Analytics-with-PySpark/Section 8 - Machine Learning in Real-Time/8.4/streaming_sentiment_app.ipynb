{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# SETTINGS\n",
    "IN_PATH = \"/home/jovyan/data-sets/twitter/\"\n",
    "MODEL_PATH = \"/home/jovyan/data-sets/sentiment-140-training-data/MODEL\"\n",
    "timestampformat = \"EEE MMM dd HH:mm:ss zzzz yyyy\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingSentiment\").getOrCreate()\n",
    "schema = spark.read.json(IN_PATH).limit(10).schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload our Pre-Trained Model \n",
    "Using ML Persistence, we simply reload the pre-trained model that we stored from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = PipelineModel.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkReader\n",
    "Select between DataStreamReader or DataReader instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static spark reader\n",
    "# spark_reader = spark.read.schema(schema)\n",
    "\n",
    "# streaming spark reader\n",
    "spark_reader = spark.readStream.schema(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Bring back our cleansing function from earlier (slightly extended with our findings from the modelling step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def html_unescape(s: str):\n",
    "    if isinstance(s, str):\n",
    "        return html.unescape(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_data(df: DataFrame):\n",
    "    url_regex = r\"((https?|ftp|file):\\/{2,3})+([-\\w+&@#/%=~|$?!:,.]*)|(www.)+([-\\w+&@#/%=~|$?!:,.]*)\"\n",
    "    email_regex = r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\"\n",
    "    user_regex = r\"(@\\w{1,15})\"\n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        \n",
    "        # Store the original text column in a new column for future reference\n",
    "        .withColumn(\"original_text\", f.col(\"text\"))\n",
    "        \n",
    "        # Remove email addresses, URLs, and user mentions\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), url_regex, \"\"))\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), email_regex, \"\"))\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), user_regex, \"\"))\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"#\", \" \"))\n",
    "        \n",
    "        # Unescape any HTML\n",
    "        .withColumn(\"text\", html_unescape(f.col(\"text\")))\n",
    "        \n",
    "        # Remove all numbers, double/multiple spaces, and leading/trailing whitespaces\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"[^a-zA-Z']\", \" \"))\n",
    "        .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \" +\", \" \"))\n",
    "        .withColumn(\"text\", f.trim(f.col(\"text\")))\n",
    "        \n",
    "        # Ensure we don't end up with empty rows\n",
    "        .filter(f.col(\"text\") != \"\").na.drop(subset=\"text\")\n",
    "    )\n",
    "\n",
    "data_in = clean_data(\n",
    "    spark_reader.json(IN_PATH)\n",
    "    .select(\n",
    "        \"id\",\n",
    "        # extract proper timestamp from created_at column\n",
    "        f.to_timestamp(f.col(\"created_at\"), timestampformat).alias(\"timestamp\"),\n",
    "        # extract user information\n",
    "        f.col(\"user.screen_name\").alias(\"user\"),\n",
    "        \"text\",\n",
    "    )\n",
    "    .coalesce(1)\n",
    ")\n",
    "\n",
    "if not data_in.isStreaming: \n",
    "    display(data_in.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentiment = sentiment_model.transform(data_in)\n",
    "\n",
    "# Select downstream columns\n",
    "sentiment = raw_sentiment.select(\n",
    "    \"id\", \"timestamp\", \"user\", \"text\", f.col(\"prediction\").alias(\"user_sentiment\")\n",
    ")\n",
    "\n",
    "if not data_in.isStreaming: \n",
    "    display(sentiment.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Queries\n",
    "Now let's define some (simple) queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sentiment_count = (\n",
    "    sentiment.filter(\"user_sentiment == 0.0\")\n",
    "    .select(f.col(\"user_sentiment\").alias(\"negative_sentiment\"))\n",
    "    .agg(f.count(\"negative_sentiment\"))\n",
    ")\n",
    "\n",
    "positive_sentiment_count = (\n",
    "    sentiment.filter(\"user_sentiment == 4.0\")\n",
    "    .select(f.col(\"user_sentiment\").alias(\"positive_sentiment\"))\n",
    "    .agg(f.count(\"positive_sentiment\"))\n",
    ")\n",
    "\n",
    "average_sentiment = sentiment.agg(f.avg(\"user_sentiment\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select what to stream\n",
    "Spark is more than capable of handling multiple streams at once, but since we are running on a small lab environment, we'll do one at a time.\n",
    "\n",
    "Make your selection by running only 1 of the next 3 cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_stream = negative_sentiment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_stream = positive_sentiment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_stream = average_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Streaming\n",
    "Now we can create our stream..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(spark_reader, DataStreamReader):\n",
    "    stream_writer = (\n",
    "        data_to_stream.writeStream.queryName(\"streaming_table\")\n",
    "        .trigger(processingTime=\"20 seconds\")\n",
    "        # .trigger(once=True)\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"memory\")\n",
    "    )\n",
    "    # Calling .start on a DataStreamWriter return an instance of StreamingQuery\n",
    "    query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Output\n",
    "...look at it's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .lastProgress shows information on the last processed batch\n",
    "if data_in.isStreaming:\n",
    "    display(query.lastProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we are outputting\n",
    "if data_in.isStreaming:\n",
    "    from time import sleep\n",
    "    for x in range(0, 200):\n",
    "        try:\n",
    "            if not query.isActive:\n",
    "                break\n",
    "            print(\"Showing live view refreshed every 10 seconds\")\n",
    "            print(f\"Seconds passed: {x*10}\")\n",
    "            result = spark.sql(f\"SELECT * from {query.name}\")\n",
    "            # spark.sql can be used to request how the query is performing\n",
    "            display(result.toPandas())\n",
    "            sleep(10)\n",
    "            clear_output(wait=True)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "    print(\"Live view ended...\")\n",
    "else:\n",
    "    print(\"Not streaming, showing static output instead\")\n",
    "    result = data_to_stream\n",
    "    display(result.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage our stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_in.isStreaming:\n",
    "    display(query.isActive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_in.isStreaming:\n",
    "    # .stop() stops the query\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start / restart our stream\n",
    "if data_in.isStreaming:\n",
    "    try:\n",
    "        # query needs to be stopped before starting a new one.\n",
    "        query.stop()\n",
    "    except NameError:\n",
    "        pass  # if query does not exist yet, we can ignore having to stop it\n",
    "    finally:\n",
    "        query = stream_writer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up behind ourselves\n",
    "Once done, make sure to stop the SparkSession, to avoid memory getting full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
