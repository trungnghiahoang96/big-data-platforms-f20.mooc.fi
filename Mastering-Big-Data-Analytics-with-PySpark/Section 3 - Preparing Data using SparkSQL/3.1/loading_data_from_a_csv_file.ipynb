{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to this tutorial\n",
    "\n",
    "This tutorial is an introduction to how to load data into Spark. For this tutorial we are going to be using the following Data Set:  \n",
    "__ratings.csv__: _100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users._\n",
    "\n",
    "This Data Set is in CSV format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession and Settings\n",
    "Before we continue, set up a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: [SPARK_LOCAL_CONFIGS]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nghiaht7/.sdkman/candidates/spark/3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"31LoadingDataFromCSV\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we have to define some settings to ensure proper operations.\n",
    " - `RATINGS_CSV_LOCATION` is used to tell our Spark Application where to find the ratings.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nghiaht7/data-engineer/big-data-platforms-f20.mooc.fi/Mastering-Big-Data-Analytics-with-PySpark/Section 3 - Preparing Data using SparkSQL/3.1\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the ratings.csv  file\n",
    "# RATINGS_CSV_LOCATION = \"/home/jovyan/data-sets/ml-latest-small/ratings.csv\"\n",
    "\n",
    "RATINGS_CSV_LOCATION = \"/home/nghiaht7/data-engineer/big-data-platforms-f20.mooc.fi/Mastering-Big-Data-Analytics-with-PySpark/data-sets/ml-latest-small/ratings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Loading Data from a CSV file\n",
    "Take a moment to study the readme file that belongs to the MovieLens-latest-small dataset.\n",
    "\n",
    "[`pyspark.sql.DataFrameReader.read.csv`](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) is used for reading csv files.\n",
    "\n",
    "We can access this, simply, by referencing our `SparkSession`, which we initated as an object we named `spark` in the previous cell. Hence, [`spark.read.csv()`](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) is used to tell Spark to read from a csv file from a given location.\n",
    "\n",
    "So let's try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(RATINGS_CSV_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now told Spark to load the data from the given CSV file. \n",
    "Because Spark is lazy, we have to explicitly tell it to show us something. \n",
    "\n",
    "Let's see the content by running `.show()` on our new DataFrame.  \n",
    "Let's also check the schema of what we loaded, by using `.printSchema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|   _c0|    _c1|   _c2|      _c3|\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see, is that the data is being loaded, but it does not quite appear to be right. Additionally, all the columns appear to be cast as a StringType - which is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the CSV file correctly and DataTypes()\n",
    "\n",
    "\n",
    "We can fix the aformentioned issues by giving the `read.csv()` method the correct settings.\n",
    "\n",
    "To quote the `README.txt` that belongs to the MovieLens data:\n",
    "> The dataset files are written as [__comma__-separated values](http://en.wikipedia.org/wiki/Comma-separated_values) files with a __single header row__. Columns that contain commas (`,`) are __escaped using double-quotes__ (`\"`). These files are __encoded as UTF-8__.\n",
    "\n",
    "*__NOTE__: It is a good idea to read the full `README.txt`, since it explains in detail how the data should be interpreted.*  \n",
    "*__NOTE__: Take a moment to study the [documentation for `read.csv()`](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) to learn about which possible things we can set.*\n",
    "\n",
    "To parse the CSV correctly, we are going to need to set the following on our `read.csv()` method:\n",
    " \n",
    " 1. We leave the same `path` as before, referring to `RATINGS_CSV_LOCATION` that we set previously.\n",
    " 2. Since we have __comma-seperated-values__, we need to set `sep` to `','`.\n",
    " 3. Since we have a __single header row__, we need to set `header` to `True`.\n",
    " 4. Since columns that contain commas (`,`) are __escaped using double-quotes__ (`\"`), we set `quote` to `'\"'`.\n",
    " 5. Since the files are __encoded as UTF-8__, we set `encoding` to `UTF-8`.\n",
    " 6. Additionally, since we observed that all values are cast to `StringType` by default, we set `inferSchema` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading CSV file with proper parsing and inferSchema\n",
    "df = spark.read.csv(\n",
    "    path=RATINGS_CSV_LOCATION,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    encoding=\"UTF-8\",\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "# Displaying results of the load\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output we can notice a few things:\n",
    "\n",
    " - The header now appears properly parsed, no more `_c0`, `_c1`, etc.\n",
    " - The numeric value columns are cast to `IntegerType` and `DoubleType` thanks to `inferSchema`\n",
    " \n",
    "Using `inferSchema`, Spark casted the following types to our schema:\n",
    "\n",
    "> `|-- userId: integer (nullable = true)`  \n",
    "> `|-- movieId: integer (nullable = true)`  \n",
    "> `|-- rating: double (nullable = true)`  \n",
    "> `|-- timestamp: integer (nullable = true)`  \n",
    "\n",
    "In short, our data now appears to have a correct parsed schema with DataTypes that appear to match the current data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Safety\n",
    "\n",
    "InferSchema is a great way to (quickly) set the schema for the data we are using. It is however good practice to be as explicit as possible when it comes to DataTypes and Schema - we call this [Type Safety](https://en.wikipedia.org/wiki/Type_safety).  \n",
    "Applying proper schema and ensuring Type Safety, is extra important once we start using more than one Data Source. For example, when trying to join two datasets, the join will not work as expected if the DataTypes of the join columns are not set correctly.\n",
    "\n",
    "*__NOTE__: Take a moment to read about [Spark DataTypes](https://spark.apache.org/docs/latest/sql-reference.html#data-types).*\n",
    "\n",
    "Let's now set out schema to an explicit value. We will do this by using the `schema` option belonging to [`read.csv()`](https://spark.apache.org/docs/2.4.3/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader).\n",
    "\n",
    "`schema`'s description reads:\n",
    "\n",
    "> *an optional `pyspark.sql.types.StructType` for the input schema or a DDL-formatted string (For example `col0 INT, col1 DOUBLE`).*\n",
    "\n",
    "I won't cover `StructType` at this point in time, but we will be using a `DDL-formatted string`. Spark uses Apache Hive's DDL language.\n",
    "\n",
    "*__NOTE__: Take a moment to read about [Compatibility with Apache Hive](https://spark.apache.org/docs/2.4.3/sql-migration-guide-hive-compatibility.html#supported-hive-features) if you want to learn more about the Apache Hive DDL syntax that Spark uses.*\n",
    "\n",
    "In this case we will define our `DDL-formatted string` as:  \n",
    "`'userId INT, movieId INT, rating DOUBLE, timestamp INT'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "|summary|            userId|         movieId|            rating|           timestamp|\n",
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "|  count|            100836|          100836|            100836|              100836|\n",
      "|   mean|326.12756356856676|19435.2957177992| 3.501556983616962|1.2059460873684695E9|\n",
      "| stddev| 182.6184914635004|35530.9871987003|1.0425292390606342|2.1626103599513078E8|\n",
      "|    min|                 1|               1|               0.5|           828124615|\n",
      "|    max|               610|          193609|               5.0|          1537799250|\n",
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [userId#135,movieId#136,rating#137,timestamp#138] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/nghiaht7/data-engineer/big-data-platforms-f20.mooc.fi/Mastering-Big-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<userId:int,movieId:int,rating:double,timestamp:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Type safe loading of ratings.csv file\n",
    "df = spark.read.csv(\n",
    "    path=RATINGS_CSV_LOCATION,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    encoding=\"UTF-8\",\n",
    "    schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    ")\n",
    "\n",
    "# Displaying results of the load\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.describe().show()\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the same output as before, but since we have an explicit schema we can ensure Type Safety\n",
    "\n",
    "## What we've learned so far:\n",
    "\n",
    "- How to use `read.csv()` to load CSV files, and how to control the settings of this method\n",
    "- By default, CSVs are parsed with all columns being cast to `StringType`\n",
    "- `inferSchema` allows Spark to guess what schema should be used\n",
    "- To ensure proper Type Safety, we can use Hive Schema DDL to set an explicit schema\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of part 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
