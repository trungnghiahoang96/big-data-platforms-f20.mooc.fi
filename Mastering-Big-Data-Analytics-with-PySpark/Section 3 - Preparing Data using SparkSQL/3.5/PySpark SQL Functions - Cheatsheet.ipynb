{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"section\" id=\"module-pyspark.sql.functions\">\n",
    "<span id=\"pyspark-sql-functions-module\"></span><h2>pyspark.sql.functions module<a class=\"headerlink\" href=\"#module-pyspark.sql.functions\" title=\"Permalink to this headline\" style=\"outline-width: 0px !important; user-select: auto !important;\">Â¶</a></h2>\n",
    "<p>A collections of builtin functions</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Functions\n",
    "\n",
    "| Function Name | Description | Since Version |\n",
    "| --- | --- | --- |\n",
    "| <a href=\"#pyspark.sql.functions.approx_count_distinct\">approx_count_distinct</a> | returns a new `Column` for approximate distinct count of column `col`. | 2.1 |\n",
    "| <a href=\"#pyspark.sql.functions.avg\">avg</a> | returns the average of the values in a group. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.collect_list\">collect_list</a> | returns a list of objects with duplicates. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.collect_set\">collect_set</a> | returns a set of objects with duplicate elements eliminated. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.count\">count</a> | returns the number of items in a group. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.countDistinct\">countDistinct</a> | Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for distinct count of <code class=\"docutils literal notranslate\"><span class=\"pre\">col</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">cols</span></code>. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.corr\">corr</a> | Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for the Pearson Correlation Coefficient for <code class=\"docutils literal notranslate\"><span class=\"pre\">col1</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">col2</span></code>. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.covar_pop\">covar_pop</a> | Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for the population covariance of <code class=\"docutils literal notranslate\"><span class=\"pre\">col1</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">col2</span></code>. | 2.0 |\n",
    "| <a href=\"#pyspark.sql.functions.covar_samp\">covar_samp</a> | Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for the sample covariance of <code class=\"docutils literal notranslate\"><span class=\"pre\">col1</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">col2</span></code>. | 2.0 |\n",
    "| <a href=\"#pyspark.sql.functions.first\">first</a> | returns the first value in a group. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.grouping\">grouping</a> | indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set. | 2.0 |\n",
    "| <a href=\"#pyspark.sql.functions.grouping_id\">grouping_id</a> | returns the level of grouping, equals to | 2.0 |\n",
    "| <a href=\"#pyspark.sql.functions.kurtosis\">kurtosis</a> | returns the kurtosis of the values in a group. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.last\">last</a> | returns the last value in a group. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.max\">max</a> | returns the maximum value of the expression in a group. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.mean\">mean</a> | returns the average of the values in a group. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.min\">min</a> | returns the minimum value of the expression in a group. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.skewness\">skewness</a> | returns the skewness of the values in a group. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.stddev\">stddev</a> | returns the unbiased sample standard deviation of the expression in a group. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.stddev_pop\">stddev_pop</a> | returns population standard deviation of the expression in a group. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.stddev_samp\">stddev_samp</a> | returns the unbiased sample standard deviation of the expression in a group. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.sum\">sum</a> | returns the sum of all values in the expression. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.sumDistinct\">sumDistinct</a> | returns the sum of distinct values in the expression. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.var_pop\">var_pop</a> | returns the population variance of the values in a group. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.var_samp\">var_samp</a> | returns the unbiased variance of the values in a group. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.variance\">variance</a> | returns the population variance of the values in a group. | 1.6 |\n",
    "\n",
    "## All other functions\n",
    "\n",
    "| Function Name | Description | Since Version |\n",
    "| --- | --- | --- |\n",
    "| <a href=\"#pyspark.sql.functions.PandasUDFType\">PandasUDFType</a> | Pandas UDF Types. See <a class=\"reference internal\" href=\"#pyspark.sql.functions.pandas_udf\" title=\"pyspark.sql.functions.pandas_udf\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">pyspark.sql.functions.pandas_udf()</span></code></a>. |  |\n",
    "| <a href=\"#pyspark.sql.functions.abs\">abs</a> | Computes the absolute value. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.acos\">acos</a> | inverse cosine of <cite>col</cite>, as if computed by <cite>java.lang.Math.acos()</cite> | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.add_months\">add_months</a> | Returns the date that is <cite>months</cite> months after <cite>start</cite> | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.array\">array</a> | Creates a new array column. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_contains\">array_contains</a> | Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.array_distinct\">array_distinct</a> | Collection function: removes duplicate values from the array. :param col: name of column or expression | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_except\">array_except</a> | Collection function: returns an array of the elements in col1 but not in col2, without duplicates. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_intersect\">array_intersect</a> | Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_join\">array_join</a> | Concatenates the elements of <cite>column</cite> using the <cite>delimiter</cite>. Null values are replaced with <cite>null_replacement</cite> if set, otherwise they are ignored. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_max\">array_max</a> | Collection function: returns the maximum value of the array. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_min\">array_min</a> | Collection function: returns the minimum value of the array. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_position\">array_position</a> | Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_remove\">array_remove</a> | Collection function: Remove all elements that equal to element from the given array. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_repeat\">array_repeat</a> | Collection function: creates an array containing a column repeated count times. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_sort\">array_sort</a> | Collection function: sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.array_union\">array_union</a> | Collection function: returns an array of the elements in the union of col1 and col2, without duplicates. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.arrays_overlap\">arrays_overlap</a> | Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.arrays_zip\">arrays_zip</a> | Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.asc\">asc</a> | Returns a sort expression based on the ascending order of the given column name. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.asc_nulls_first\">asc_nulls_first</a> | Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.asc_nulls_last\">asc_nulls_last</a> | Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.ascii\">ascii</a> | Computes the numeric value of the first character of the string column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.asin\">asin</a> | inverse sine of <cite>col</cite>, as if computed by <cite>java.lang.Math.asin()</cite> | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.atan\">atan</a> | inverse tangent of <cite>col</cite>, as if computed by <cite>java.lang.Math.atan()</cite> | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.atan2\">atan2</a> | <strong>col1</strong> â coordinate on y-axis | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.base64\">base64</a> | Computes the BASE64 encoding of a binary column and returns it as a string column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.basestring\">basestring</a> | alias of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">builtins.str</span></code> |  |\n",
    "| <a href=\"#pyspark.sql.functions.bin\">bin</a> | Returns the string representation of the binary value of the given column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.bitwiseNOT\">bitwiseNOT</a> | Computes bitwise not. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.broadcast\">broadcast</a> | Marks a DataFrame as small enough for use in broadcast joins. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.bround\">bround</a> | Round the given value to <cite>scale</cite> decimal places using HALF_EVEN rounding mode if <cite>scale</cite> &gt;= 0 or at integral part when <cite>scale</cite> &lt; 0. | 2.0 |\n",
    "| <a href=\"#pyspark.sql.functions.cbrt\">cbrt</a> | Computes the cube-root of the given value. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.ceil\">ceil</a> | Computes the ceiling of the given value. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.coalesce\">coalesce</a> | Returns the first column that is not null. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.col\">col</a> | Returns a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> based on the given column name. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.column\">column</a> | Returns a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> based on the given column name. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.concat\">concat</a> | Concatenates multiple input columns together into a single column. The function works with strings, binary and compatible array columns. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.concat_ws\">concat_ws</a> | Concatenates multiple input string columns together into a single string column, using the given separator. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.conv\">conv</a> | Convert a number in a string column from one base to another. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.cos\">cos</a> | <strong>col</strong> â angle in radians | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.cosh\">cosh</a> | <strong>col</strong> â hyperbolic angle | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.crc32\">crc32</a> | Calculates the cyclic redundancy check value  (CRC32) of a binary column and returns the value as a bigint. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.create_map\">create_map</a> | Creates a new map column. | 2.0 |\n",
    "| <a href=\"#pyspark.sql.functions.cume_dist\">cume_dist</a> | Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.current_date\">current_date</a> | Returns the current date as a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DateType</span></code> column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.current_timestamp\">current_timestamp</a> | Returns the current timestamp as a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">TimestampType</span></code> column. |  |\n",
    "| <a href=\"#pyspark.sql.functions.date_add\">date_add</a> | Returns the date that is <cite>days</cite> days after <cite>start</cite> | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.date_format\">date_format</a> | Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.date_sub\">date_sub</a> | Returns the date that is <cite>days</cite> days before <cite>start</cite> | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.date_trunc\">date_trunc</a> | Returns timestamp truncated to the unit specified by the format. | 2.3 |\n",
    "| <a href=\"#pyspark.sql.functions.datediff\">datediff</a> | Returns the number of days from <cite>start</cite> to <cite>end</cite>. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.dayofmonth\">dayofmonth</a> | Extract the day of the month of a given date as integer. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.dayofweek\">dayofweek</a> | Extract the day of the week of a given date as integer. | 2.3 |\n",
    "| <a href=\"#pyspark.sql.functions.dayofyear\">dayofyear</a> | Extract the day of the year of a given date as integer. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.decode\">decode</a> | Computes the first argument into a string from a binary using the provided character set (one of âUS-ASCIIâ, âISO-8859-1â, âUTF-8â, âUTF-16BEâ, âUTF-16LEâ, âUTF-16â). | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.degrees\">degrees</a> | Converts an angle measured in radians to an approximately equivalent angle measured in degrees. :param col: angle in radians :return: angle in degrees, as if computed by <cite>java.lang.Math.toDegrees()</cite> | 2.1 |\n",
    "| <a href=\"#pyspark.sql.functions.dense_rank\">dense_rank</a> | Window function: returns the rank of rows within a window partition, without any gaps. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.desc\">desc</a> | Returns a sort expression based on the descending order of the given column name. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.desc_nulls_first\">desc_nulls_first</a> | Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.desc_nulls_last\">desc_nulls_last</a> | Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.element_at\">element_at</a> | Collection function: Returns element of array at given index in extraction if col is array. Returns value for the given key in extraction if col is map. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.encode\">encode</a> | Computes the first argument into a binary from a string using the provided character set (one of âUS-ASCIIâ, âISO-8859-1â, âUTF-8â, âUTF-16BEâ, âUTF-16LEâ, âUTF-16â). | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.exp\">exp</a> | Computes the exponential of the given value. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.explode\">explode</a> | Returns a new row for each element in the given array or map. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.explode_outer\">explode_outer</a> | Returns a new row for each element in the given array or map. Unlike explode, if the array/map is null or empty then null is produced. | 2.3 |\n",
    "| <a href=\"#pyspark.sql.functions.expm1\">expm1</a> | Computes the exponential of the given value minus one. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.expr\">expr</a> | Parses the expression string into the column that it represents | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.factorial\">factorial</a> | Computes the factorial of the given value. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.flatten\">flatten</a> | Collection function: creates a single array from an array of arrays. If a structure of nested arrays is deeper than two levels, only one level of nesting is removed. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.floor\">floor</a> | Computes the floor of the given value. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.format_number\">format_number</a> | Formats the number X to a format like â#,â#,â#.ââ, rounded to d decimal places with HALF_EVEN round mode, and returns the result as a string. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.format_string\">format_string</a> | Formats the arguments in printf-style and returns the result as a string column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.from_json\">from_json</a> | Parses a column containing a JSON string into a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MapType</span></code> with <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StringType</span></code> as keys type, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StructType</span></code> or <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ArrayType</span></code> with the specified schema. Returns <cite>null</cite>, in the case of an unparseable string. | 2.1 |\n",
    "| <a href=\"#pyspark.sql.functions.from_unixtime\">from_unixtime</a> | Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.from_utc_timestamp\">from_utc_timestamp</a> | This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and renders that timestamp as a timestamp in the given time zone. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.get_json_object\">get_json_object</a> | Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.greatest\">greatest</a> | Returns the greatest value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.hash\">hash</a> | Calculates the hash code of given columns, and returns the result as an int column. | 2.0 |\n",
    "| <a href=\"#pyspark.sql.functions.hex\">hex</a> | Computes hex value of the given column, which could be <a class=\"reference internal\" href=\"#pyspark.sql.types.StringType\" title=\"pyspark.sql.types.StringType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.StringType</span></code></a>, <a class=\"reference internal\" href=\"#pyspark.sql.types.BinaryType\" title=\"pyspark.sql.types.BinaryType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.BinaryType</span></code></a>, <a class=\"reference internal\" href=\"#pyspark.sql.types.IntegerType\" title=\"pyspark.sql.types.IntegerType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.IntegerType</span></code></a> or <a class=\"reference internal\" href=\"#pyspark.sql.types.LongType\" title=\"pyspark.sql.types.LongType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.LongType</span></code></a>. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.hour\">hour</a> | Extract the hours of a given date as integer. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.hypot\">hypot</a> | Computes <code class=\"docutils literal notranslate\"><span class=\"pre\">sqrt(a^2</span> <span class=\"pre\">+</span> <span class=\"pre\">b^2)</span></code> without intermediate overflow or underflow. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.initcap\">initcap</a> | Translate the first letter of each word to upper case in the sentence. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.input_file_name\">input_file_name</a> | Creates a string column for the file name of the current Spark task. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.instr\">instr</a> | Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.isnan\">isnan</a> | An expression that returns true iff the column is NaN. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.isnull\">isnull</a> | An expression that returns true iff the column is null. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.json_tuple\">json_tuple</a> | Creates a new row for a json column according to the given field names. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.lag\">lag</a> | Window function: returns the value that is <cite>offset</cite> rows before the current row, and <cite>defaultValue</cite> if there is less than <cite>offset</cite> rows before the current row. For example, an <cite>offset</cite> of one will return the previous row at any given point in the window partition. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.last_day\">last_day</a> | Returns the last day of the month which the given date belongs to. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.lead\">lead</a> | Window function: returns the value that is <cite>offset</cite> rows after the current row, and <cite>defaultValue</cite> if there is less than <cite>offset</cite> rows after the current row. For example, an <cite>offset</cite> of one will return the next row at any given point in the window partition. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.least\">least</a> | Returns the least value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.length\">length</a> | Computes the character length of string data or number of bytes of binary data. The length of character data includes the trailing spaces. The length of binary data includes binary zeros. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.levenshtein\">levenshtein</a> | Computes the Levenshtein distance of the two given strings. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.lit\">lit</a> | Creates a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> of literal value. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.locate\">locate</a> | Locate the position of the first occurrence of substr in a string column, after position pos. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.log\">log</a> | Returns the first argument-based logarithm of the second argument. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.log10\">log10</a> | Computes the logarithm of the given value in Base 10. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.log1p\">log1p</a> | Computes the natural logarithm of the given value plus one. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.log2\">log2</a> | Returns the base-2 logarithm of the argument. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.lower\">lower</a> | Converts a string column to lower case. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.lpad\">lpad</a> | Left-pad the string column to width <cite>len</cite> with <cite>pad</cite>. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.ltrim\">ltrim</a> | Trim the spaces from left end for the specified string value. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.map_concat\">map_concat</a> | Returns the union of all the given maps. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.map_from_arrays\">map_from_arrays</a> | Creates a new map from two arrays. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.map_from_entries\">map_from_entries</a> | Collection function: Returns a map created from the given array of entries. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.map_keys\">map_keys</a> | Collection function: Returns an unordered array containing the keys of the map. | 2.3 |\n",
    "| <a href=\"#pyspark.sql.functions.map_values\">map_values</a> | Collection function: Returns an unordered array containing the values of the map. | 2.3 |\n",
    "| <a href=\"#pyspark.sql.functions.md5\">md5</a> | Calculates the MD5 digest and returns the value as a 32 character hex string. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.minute\">minute</a> | Extract the minutes of a given date as integer. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.monotonically_increasing_id\">monotonically_increasing_id</a> | A column that generates monotonically increasing 64-bit integers. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.month\">month</a> | Extract the month of a given date as integer. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.months_between\">months_between</a> | Returns number of months between dates date1 and date2. If date1 is later than date2, then the result is positive. If date1 and date2 are on the same day of month, or both are the last day of month, returns an integer (time of day will be ignored). The result is rounded off to 8 digits unless <cite>roundOff</cite> is set to <cite>False</cite>. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.nanvl\">nanvl</a> | Returns col1 if it is not NaN, or col2 if col1 is NaN. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.next_day\">next_day</a> | Returns the first date which is later than the value of the date column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.ntile\">ntile</a> | Window function: returns the ntile group id (from 1 to <cite>n</cite> inclusive) in an ordered window partition. For example, if <cite>n</cite> is 4, the first quarter of the rows will get value 1, the second quarter will get 2, the third quarter will get 3, and the last quarter will get 4. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.pandas_udf\">pandas_udf</a> | Creates a vectorized user defined function (UDF). | 2.3 |\n",
    "| <a href=\"#pyspark.sql.functions.percent_rank\">percent_rank</a> | Window function: returns the relative rank (i.e. percentile) of rows within a window partition. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.posexplode\">posexplode</a> | Returns a new row for each element with position in the given array or map. | 2.1 |\n",
    "| <a href=\"#pyspark.sql.functions.posexplode_outer\">posexplode_outer</a> | Returns a new row for each element with position in the given array or map. Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced. | 2.3 |\n",
    "| <a href=\"#pyspark.sql.functions.pow\">pow</a> | Returns the value of the first argument raised to the power of the second argument. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.quarter\">quarter</a> | Extract the quarter of a given date as integer. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.radians\">radians</a> | Converts an angle measured in degrees to an approximately equivalent angle measured in radians. :param col: angle in degrees :return: angle in radians, as if computed by <cite>java.lang.Math.toRadians()</cite> | 2.1 |\n",
    "| <a href=\"#pyspark.sql.functions.rand\">rand</a> | Generates a random column with independent and identically distributed (i.i.d.) samples from U[0.0, 1.0]. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.randn\">randn</a> | Generates a column with independent and identically distributed (i.i.d.) samples from the standard normal distribution. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.rank\">rank</a> | Window function: returns the rank of rows within a window partition. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.regexp_extract\">regexp_extract</a> | Extract a specific group matched by a Java regex, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.regexp_replace\">regexp_replace</a> | Replace all substrings of the specified string value that match regexp with rep. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.repeat\">repeat</a> | Repeats a string column n times, and returns it as a new string column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.reverse\">reverse</a> | Collection function: returns a reversed string or an array with reverse order of elements. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.rint\">rint</a> | Returns the double value that is closest in value to the argument and is equal to a mathematical integer. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.round\">round</a> | Round the given value to <cite>scale</cite> decimal places using HALF_UP rounding mode if <cite>scale</cite> &gt;= 0 or at integral part when <cite>scale</cite> &lt; 0. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.row_number\">row_number</a> | Window function: returns a sequential number starting at 1 within a window partition. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.rpad\">rpad</a> | Right-pad the string column to width <cite>len</cite> with <cite>pad</cite>. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.rtrim\">rtrim</a> | Trim the spaces from right end for the specified string value. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.schema_of_json\">schema_of_json</a> | Parses a JSON string and infers its schema in DDL format. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.second\">second</a> | Extract the seconds of a given date as integer. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.sequence\">sequence</a> | Generate a sequence of integers from <cite>start</cite> to <cite>stop</cite>, incrementing by <cite>step</cite>. If <cite>step</cite> is not set, incrementing by 1 if <cite>start</cite> is less than or equal to <cite>stop</cite>, otherwise -1. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.sha1\">sha1</a> | Returns the hex string result of SHA-1. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.sha2\">sha2</a> | Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512). The numBits indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.shiftLeft\">shiftLeft</a> | Shift the given value numBits left. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.shiftRight\">shiftRight</a> | (Signed) shift the given value numBits right. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.shiftRightUnsigned\">shiftRightUnsigned</a> | Unsigned shift the given value numBits right. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.shuffle\">shuffle</a> | Collection function: Generates a random permutation of the given array. | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.signum\">signum</a> | Computes the signum of the given value. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.sin\">sin</a> | <strong>col</strong> â angle in radians | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.sinh\">sinh</a> | <strong>col</strong> â hyperbolic angle | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.size\">size</a> | Collection function: returns the length of the array or map stored in the column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.slice\">slice</a> | Collection function: returns an array containing  all the elements in <cite>x</cite> from index <cite>start</cite> (or starting from the end if <cite>start</cite> is negative) with the specified <cite>length</cite>. &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], [âxâ]) &gt;&gt;&gt; df.select(slice(df.x, 2, 2).alias(âslicedâ)).collect() [Row(sliced=[2, 3]), Row(sliced=[5])] | 2.4 |\n",
    "| <a href=\"#pyspark.sql.functions.sort_array\">sort_array</a> | Collection function: sorts the input array in ascending or descending order according to the natural ordering of the array elements. Null elements will be placed at the beginning of the returned array in ascending order or at the end of the returned array in descending order. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.soundex\">soundex</a> | Returns the SoundEx encoding for a string | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.spark_partition_id\">spark_partition_id</a> | A column for partition ID. | 1.6 |\n",
    "| <a href=\"#pyspark.sql.functions.split\">split</a> | Splits str around pattern (pattern is a regular expression). | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.sqrt\">sqrt</a> | Computes the square root of the specified float value. | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.struct\">struct</a> | Creates a new struct column. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.substring\">substring</a> | Substring starts at <cite>pos</cite> and is of length <cite>len</cite> when str is String type or returns the slice of byte array that starts at <cite>pos</cite> in byte and is of length <cite>len</cite> when str is Binary type. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.substring_index\">substring_index</a> | Returns the substring from string str before count occurrences of the delimiter delim. If count is positive, everything the left of the final delimiter (counting from left) is returned. If count is negative, every to the right of the final delimiter (counting from the right) is returned. substring_index performs a case-sensitive match when searching for delim. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.tan\">tan</a> | <strong>col</strong> â angle in radians | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.tanh\">tanh</a> | <strong>col</strong> â hyperbolic angle | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.to_date\">to_date</a> | Converts a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> of <a class=\"reference internal\" href=\"#pyspark.sql.types.StringType\" title=\"pyspark.sql.types.StringType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.StringType</span></code></a> or <a class=\"reference internal\" href=\"#pyspark.sql.types.TimestampType\" title=\"pyspark.sql.types.TimestampType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.TimestampType</span></code></a> into <a class=\"reference internal\" href=\"#pyspark.sql.types.DateType\" title=\"pyspark.sql.types.DateType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.DateType</span></code></a> using the optionally specified format. Specify formats according to <a class=\"reference external\" href=\"http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\">SimpleDateFormats</a>. By default, it follows casting rules to <a class=\"reference internal\" href=\"#pyspark.sql.types.DateType\" title=\"pyspark.sql.types.DateType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.DateType</span></code></a> if the format is omitted (equivalent to <code class=\"docutils literal notranslate\"><span class=\"pre\">col.cast(\"date\")</span></code>). | 2.2 |\n",
    "| <a href=\"#pyspark.sql.functions.to_json\">to_json</a> | Converts a column containing a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StructType</span></code>, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ArrayType</span></code> or a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MapType</span></code> into a JSON string. Throws an exception, in the case of an unsupported type. | 2.1 |\n",
    "| <a href=\"#pyspark.sql.functions.to_timestamp\">to_timestamp</a> | Converts a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> of <a class=\"reference internal\" href=\"#pyspark.sql.types.StringType\" title=\"pyspark.sql.types.StringType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.StringType</span></code></a> or <a class=\"reference internal\" href=\"#pyspark.sql.types.TimestampType\" title=\"pyspark.sql.types.TimestampType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.TimestampType</span></code></a> into <a class=\"reference internal\" href=\"#pyspark.sql.types.DateType\" title=\"pyspark.sql.types.DateType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.DateType</span></code></a> using the optionally specified format. Specify formats according to <a class=\"reference external\" href=\"http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\">SimpleDateFormats</a>. By default, it follows casting rules to <a class=\"reference internal\" href=\"#pyspark.sql.types.TimestampType\" title=\"pyspark.sql.types.TimestampType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.TimestampType</span></code></a> if the format is omitted (equivalent to <code class=\"docutils literal notranslate\"><span class=\"pre\">col.cast(\"timestamp\")</span></code>). | 2.2 |\n",
    "| <a href=\"#pyspark.sql.functions.to_utc_timestamp\">to_utc_timestamp</a> | This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given timezone, and renders that timestamp as a timestamp in UTC. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.translate\">translate</a> | A function translate any character in the <cite>srcCol</cite> by a character in <cite>matching</cite>. The characters in <cite>replace</cite> is corresponding to the characters in <cite>matching</cite>. The translate will happen when any character in the string matching with the character in the <cite>matching</cite>. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.trim\">trim</a> | Trim the spaces from both ends for the specified string column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.trunc\">trunc</a> | Returns date truncated to the unit specified by the format. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.udf\">udf</a> | Creates a user defined function (UDF). | 1.3 |\n",
    "| <a href=\"#pyspark.sql.functions.unbase64\">unbase64</a> | Decodes a BASE64 encoded string column and returns it as a binary column. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.unhex\">unhex</a> | Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of number. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.unix_timestamp\">unix_timestamp</a> | Convert time string with given pattern (âyyyy-MM-dd HH:mm:ssâ, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.upper\">upper</a> | Converts a string column to upper case. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.weekofyear\">weekofyear</a> | Extract the week number of a given date as integer. | 1.5 |\n",
    "| <a href=\"#pyspark.sql.functions.when\">when</a> | Evaluates a list of conditions and returns one of multiple possible result expressions. If <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">Column.otherwise()</span></code> is not invoked, None is returned for unmatched conditions. | 1.4 |\n",
    "| <a href=\"#pyspark.sql.functions.window\">window</a> | Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported. | 2.0 |\n",
    "| <a href=\"#pyspark.sql.functions.year\">year</a> | Extract the year of a given date as integer. | 1.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregates\n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.approx_count_distinct\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">approx_count_distinct</code><span class=\"sig-paren\">(</span><em>col</em>, <em>rsd=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#approx_count_distinct\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.approx_count_distinct\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for approximate distinct count of column <cite>col</cite>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>rsd</strong> â maximum estimation error allowed (default = 0.05). For rsd &lt; 0.01, it is more efficient to use <a class=\"reference internal\" href=\"#pyspark.sql.functions.countDistinct\" title=\"pyspark.sql.functions.countDistinct\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">countDistinct()</span></code></a></p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">approx_count_distinct</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'distinct_ages'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(distinct_ages=2)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.1.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.avg\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">avg</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.avg\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the average of the values in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.collect_list\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">collect_list</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.collect_list\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns a list of objects with duplicates.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The function is non-deterministic because the order of collected results depends on order of rows which may be non-deterministic after a shuffle.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">2</span><span class=\"p\">,),</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,),</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,)],</span> <span class=\"p\">(</span><span class=\"s1\">'age'</span><span class=\"p\">,))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df2</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">collect_list</span><span class=\"p\">(</span><span class=\"s1\">'age'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(collect_list(age)=[2, 5, 5])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.collect_set\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">collect_set</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.collect_set\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns a set of objects with duplicate elements eliminated.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The function is non-deterministic because the order of collected results depends on order of rows which may be non-deterministic after a shuffle.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">2</span><span class=\"p\">,),</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,),</span> <span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,)],</span> <span class=\"p\">(</span><span class=\"s1\">'age'</span><span class=\"p\">,))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df2</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">collect_set</span><span class=\"p\">(</span><span class=\"s1\">'age'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(collect_set(age)=[5, 2])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.count\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">count</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.count\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the number of items in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.countDistinct\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">countDistinct</code><span class=\"sig-paren\">(</span><em>col</em>, <em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#countDistinct\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.countDistinct\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for distinct count of <code class=\"docutils literal notranslate\"><span class=\"pre\">col</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">cols</span></code>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">countDistinct</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'c'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(c=2)]</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">countDistinct</span><span class=\"p\">(</span><span class=\"s2\">\"age\"</span><span class=\"p\">,</span> <span class=\"s2\">\"name\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'c'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(c=2)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.corr\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">corr</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#corr\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.corr\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for the Pearson Correlation Coefficient for <code class=\"docutils literal notranslate\"><span class=\"pre\">col1</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">col2</span></code>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">)]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"p\">[</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">corr</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'c'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(c=1.0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.covar_pop\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">covar_pop</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#covar_pop\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.covar_pop\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for the population covariance of <code class=\"docutils literal notranslate\"><span class=\"pre\">col1</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">col2</span></code>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"mi\">10</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"mi\">10</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"p\">[</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">covar_pop</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'c'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(c=0.0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.0.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.covar_samp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">covar_samp</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#covar_samp\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.covar_samp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> for the sample covariance of <code class=\"docutils literal notranslate\"><span class=\"pre\">col1</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">col2</span></code>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"mi\">10</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"mi\">10</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">),</span> <span class=\"p\">[</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">covar_samp</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'c'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(c=0.0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.0.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.first\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">first</code><span class=\"sig-paren\">(</span><em>col</em>, <em>ignorenulls=False</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#first\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.first\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the first value in a group.</p> <p>The function by default returns the first values it sees. It will return the first non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The function is non-deterministic because its results depends on order of rows which may be non-deterministic after a shuffle.</p> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.grouping\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">grouping</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#grouping\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.grouping\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated or not, returns 1 for aggregated or 0 for not aggregated in the result set.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">cube</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">grouping</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">),</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"s2\">\"age\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+-----+--------------+--------+</span> <span class=\"go\">| name|grouping(name)|sum(age)|</span> <span class=\"go\">+-----+--------------+--------+</span> <span class=\"go\">| null|             1|       7|</span> <span class=\"go\">|Alice|             0|       2|</span> <span class=\"go\">|  Bob|             0|       5|</span> <span class=\"go\">+-----+--------------+--------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.0.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.grouping_id\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">grouping_id</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#grouping_id\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.grouping_id\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the level of grouping, equals to</p> <blockquote> <div><p>(grouping(c1) &lt;&lt; (n-1)) + (grouping(c2) &lt;&lt; (n-2)) + â¦ + grouping(cn)</p> </div></blockquote> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The list of columns should match with grouping columns exactly, or empty (means all the grouping columns).</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">cube</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">grouping_id</span><span class=\"p\">(),</span> <span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"s2\">\"age\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+-----+-------------+--------+</span> <span class=\"go\">| name|grouping_id()|sum(age)|</span> <span class=\"go\">+-----+-------------+--------+</span> <span class=\"go\">| null|            1|       7|</span> <span class=\"go\">|Alice|            0|       2|</span> <span class=\"go\">|  Bob|            0|       5|</span> <span class=\"go\">+-----+-------------+--------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.0.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.kurtosis\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">kurtosis</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.kurtosis\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the kurtosis of the values in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.last\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">last</code><span class=\"sig-paren\">(</span><em>col</em>, <em>ignorenulls=False</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#last\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.last\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the last value in a group.</p> <p>The function by default returns the last values it sees. It will return the last non-null value it sees when ignoreNulls is set to true. If all values are null, then null is returned.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The function is non-deterministic because its results depends on order of rows which may be non-deterministic after a shuffle.</p> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.max\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">max</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.max\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the maximum value of the expression in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.mean\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">mean</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.mean\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the average of the values in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.min\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">min</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.min\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the minimum value of the expression in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.skewness\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">skewness</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.skewness\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the skewness of the values in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.stddev\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">stddev</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.stddev\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the unbiased sample standard deviation of the expression in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.stddev_pop\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">stddev_pop</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.stddev_pop\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns population standard deviation of the expression in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.stddev_samp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">stddev_samp</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.stddev_samp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the unbiased sample standard deviation of the expression in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sum\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sum</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.sum\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the sum of all values in the expression.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sumDistinct\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sumDistinct</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.sumDistinct\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the sum of distinct values in the expression.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.var_pop\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">var_pop</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.var_pop\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the population variance of the values in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.var_samp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">var_samp</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.var_samp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the unbiased variance of the values in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.variance\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">variance</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.variance\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Aggregate function: returns the population variance of the values in a group.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "### Others\n",
    "\n",
    "<dl class=\"class\"> <dt id=\"pyspark.sql.functions.PandasUDFType\"> <em class=\"property\">class </em><code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">PandasUDFType</code><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#PandasUDFType\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.PandasUDFType\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Pandas UDF Types. See <a class=\"reference internal\" href=\"#pyspark.sql.functions.pandas_udf\" title=\"pyspark.sql.functions.pandas_udf\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">pyspark.sql.functions.pandas_udf()</span></code></a>.</p> <dl class=\"attribute\"> <dt id=\"pyspark.sql.functions.PandasUDFType.GROUPED_AGG\"> <code class=\"descname\">GROUPED_AGG</code><em class=\"property\"> = 202</em><a class=\"headerlink\" href=\"#pyspark.sql.functions.PandasUDFType.GROUPED_AGG\" title=\"Permalink to this definition\">Â¶</a></dt> <dd></dd></dl> <dl class=\"attribute\"> <dt id=\"pyspark.sql.functions.PandasUDFType.GROUPED_MAP\"> <code class=\"descname\">GROUPED_MAP</code><em class=\"property\"> = 201</em><a class=\"headerlink\" href=\"#pyspark.sql.functions.PandasUDFType.GROUPED_MAP\" title=\"Permalink to this definition\">Â¶</a></dt> <dd></dd></dl> <dl class=\"attribute\"> <dt id=\"pyspark.sql.functions.PandasUDFType.SCALAR\"> <code class=\"descname\">SCALAR</code><em class=\"property\"> = 200</em><a class=\"headerlink\" href=\"#pyspark.sql.functions.PandasUDFType.SCALAR\" title=\"Permalink to this definition\">Â¶</a></dt> <dd></dd></dl></dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.abs\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">abs</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.abs\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the absolute value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.acos\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">acos</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.acos\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\"><p>inverse cosine of <cite>col</cite>, as if computed by <cite>java.lang.Math.acos()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.add_months\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">add_months</code><span class=\"sig-paren\">(</span><em>start</em>, <em>months</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#add_months\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.add_months\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the date that is <cite>months</cite> months after <cite>start</cite></p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">add_months</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">dt</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'next_month'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(next_month=datetime.date(2015, 5, 8))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.approxCountDistinct\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">approxCountDistinct</code><span class=\"sig-paren\">(</span><em>col</em>, <em>rsd=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#approxCountDistinct\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.approxCountDistinct\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Deprecated in 2.1, use <a class=\"reference internal\" href=\"#pyspark.sql.functions.approx_count_distinct\" title=\"pyspark.sql.functions.approx_count_distinct\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">approx_count_distinct()</span></code></a> instead.</p> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a new array column.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>cols</strong> â list of column names (string) or list of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> expressions that have the same data type.</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"s1\">'age'</span><span class=\"p\">,</span> <span class=\"s1\">'age'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"arr\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(arr=[2, 2]), Row(arr=[5, 5])]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"arr\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(arr=[2, 2]), Row(arr=[5, 5])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_contains\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_contains</code><span class=\"sig-paren\">(</span><em>col</em>, <em>value</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_contains\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_contains\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â name of column containing array</p></li> <li><p><strong>value</strong> â value to check for in array</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c\"</span><span class=\"p\">],),</span> <span class=\"p\">([],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_contains</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_distinct\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_distinct</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_distinct\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_distinct\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: removes duplicate values from the array. :param col: name of column or expression</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],),</span> <span class=\"p\">([</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_distinct</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_except\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_except</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_except\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_except\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns an array of the elements in col1 but not in col2, without duplicates.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col1</strong> â name of column containing array</p></li> <li><p><strong>col2</strong> â name of column containing array</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql</span> <span class=\"k\">import</span> <span class=\"n\">Row</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span><span class=\"n\">Row</span><span class=\"p\">(</span><span class=\"n\">c1</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c\"</span><span class=\"p\">],</span> <span class=\"n\">c2</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"d\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"f\"</span><span class=\"p\">])])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_except</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c1</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c2</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(array_except(c1, c2)=['b'])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_intersect\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_intersect</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_intersect\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_intersect\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col1</strong> â name of column containing array</p></li> <li><p><strong>col2</strong> â name of column containing array</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql</span> <span class=\"k\">import</span> <span class=\"n\">Row</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span><span class=\"n\">Row</span><span class=\"p\">(</span><span class=\"n\">c1</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c\"</span><span class=\"p\">],</span> <span class=\"n\">c2</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"d\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"f\"</span><span class=\"p\">])])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_intersect</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c1</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c2</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(array_intersect(c1, c2)=['a', 'c'])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_join\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_join</code><span class=\"sig-paren\">(</span><em>col</em>, <em>delimiter</em>, <em>null_replacement=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_join\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_join\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Concatenates the elements of <cite>column</cite> using the <cite>delimiter</cite>. Null values are replaced with <cite>null_replacement</cite> if set, otherwise they are ignored.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c\"</span><span class=\"p\">],),</span> <span class=\"p\">([</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_join</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"s2\">\",\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"joined\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(joined='a,b,c'), Row(joined='a')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_join</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"s2\">\",\"</span><span class=\"p\">,</span> <span class=\"s2\">\"NULL\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"joined\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(joined='a,b,c'), Row(joined='a,NULL')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_max\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_max</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_max\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_max\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns the maximum value of the array.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],),</span> <span class=\"p\">([</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_max</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'max'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(max=3), Row(max=10)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_min\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_min</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_min\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_min\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns the minimum value of the array.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],),</span> <span class=\"p\">([</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_min</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'min'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(min=1), Row(min=-1)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_position\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_position</code><span class=\"sig-paren\">(</span><em>col</em>, <em>value</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_position\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_position\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The position is not zero based, but 1 based index. Returns 0 if the given value could not be found in the array.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"s2\">\"c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">],),</span> <span class=\"p\">([],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_position</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(array_position(data, a)=3), Row(array_position(data, a)=0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_remove\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_remove</code><span class=\"sig-paren\">(</span><em>col</em>, <em>element</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_remove\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_remove\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: Remove all elements that equal to element from the given array.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â name of column containing array</p></li> <li><p><strong>element</strong> â element to be removed from the array</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],),</span> <span class=\"p\">([],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_remove</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_repeat\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_repeat</code><span class=\"sig-paren\">(</span><em>col</em>, <em>count</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_repeat\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_repeat\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: creates an array containing a column repeated count times.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ab'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_repeat</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=['ab', 'ab', 'ab'])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_sort\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_sort</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_sort\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_sort\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],),([</span><span class=\"mi\">1</span><span class=\"p\">],),([],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_sort</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.array_union\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">array_union</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#array_union\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.array_union\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns an array of the elements in the union of col1 and col2, without duplicates.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col1</strong> â name of column containing array</p></li> <li><p><strong>col2</strong> â name of column containing array</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql</span> <span class=\"k\">import</span> <span class=\"n\">Row</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span><span class=\"n\">Row</span><span class=\"p\">(</span><span class=\"n\">c1</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c\"</span><span class=\"p\">],</span> <span class=\"n\">c2</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">\"c\"</span><span class=\"p\">,</span> <span class=\"s2\">\"d\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"f\"</span><span class=\"p\">])])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">array_union</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c1</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c2</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.arrays_overlap\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">arrays_overlap</code><span class=\"sig-paren\">(</span><em>a1</em>, <em>a2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#arrays_overlap\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.arrays_overlap\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c\"</span><span class=\"p\">]),</span> <span class=\"p\">([</span><span class=\"s2\">\"a\"</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c\"</span><span class=\"p\">])],</span> <span class=\"p\">[</span><span class=\"s1\">'x'</span><span class=\"p\">,</span> <span class=\"s1\">'y'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">arrays_overlap</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">y</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"overlap\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(overlap=True), Row(overlap=False)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.arrays_zip\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">arrays_zip</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#arrays_zip\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.arrays_zip\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>cols</strong> â columns of arrays to be merged.</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">arrays_zip</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">]))],</span> <span class=\"p\">[</span><span class=\"s1\">'vals1'</span><span class=\"p\">,</span> <span class=\"s1\">'vals2'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">arrays_zip</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">vals1</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">vals2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'zipped'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.asc\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">asc</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.asc\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a sort expression based on the ascending order of the given column name.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.asc_nulls_first\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">asc_nulls_first</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.asc_nulls_first\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.asc_nulls_last\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">asc_nulls_last</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.asc_nulls_last\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.ascii\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">ascii</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.ascii\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the numeric value of the first character of the string column.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.asin\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">asin</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.asin\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\"><p>inverse sine of <cite>col</cite>, as if computed by <cite>java.lang.Math.asin()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.atan\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">atan</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.atan\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\"><p>inverse tangent of <cite>col</cite>, as if computed by <cite>java.lang.Math.atan()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.atan2\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">atan2</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.atan2\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col1</strong> â coordinate on y-axis</p></li> <li><p><strong>col2</strong> â coordinate on x-axis</p></li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\"><p>the <cite>theta</cite> component of the point (<cite>r</cite>, <cite>theta</cite>) in polar coordinates that corresponds to the point (<cite>x</cite>, <cite>y</cite>) in Cartesian coordinates, as if computed by <cite>java.lang.Math.atan2()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.base64\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">base64</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.base64\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the BASE64 encoding of a binary column and returns it as a string column.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"attribute\"> <dt id=\"pyspark.sql.functions.basestring\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">basestring</code><a class=\"headerlink\" href=\"#pyspark.sql.functions.basestring\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>alias of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">builtins.str</span></code></p> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.bin\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">bin</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#bin\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.bin\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the string representation of the binary value of the given column.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"nb\">bin</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'c'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(c='10'), Row(c='101')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.bitwiseNOT\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">bitwiseNOT</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.bitwiseNOT\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes bitwise not.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.broadcast\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">broadcast</code><span class=\"sig-paren\">(</span><em>df</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#broadcast\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.broadcast\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Marks a DataFrame as small enough for use in broadcast joins.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.bround\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">bround</code><span class=\"sig-paren\">(</span><em>col</em>, <em>scale=0</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#bround\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.bround\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Round the given value to <cite>scale</cite> decimal places using HALF_EVEN rounding mode if <cite>scale</cite> &gt;= 0 or at integral part when <cite>scale</cite> &lt; 0.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mf\">2.5</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">bround</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=2.0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.0.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.cbrt\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">cbrt</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.cbrt\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the cube-root of the given value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.ceil\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">ceil</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.ceil\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the ceiling of the given value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.coalesce\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">coalesce</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#coalesce\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.coalesce\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the first column that is not null.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">cDf</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)],</span> <span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">cDf</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+----+----+</span> <span class=\"go\">|   a|   b|</span> <span class=\"go\">+----+----+</span> <span class=\"go\">|null|null|</span> <span class=\"go\">|   1|null|</span> <span class=\"go\">|null|   2|</span> <span class=\"go\">+----+----+</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">cDf</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">coalesce</span><span class=\"p\">(</span><span class=\"n\">cDf</span><span class=\"p\">[</span><span class=\"s2\">\"a\"</span><span class=\"p\">],</span> <span class=\"n\">cDf</span><span class=\"p\">[</span><span class=\"s2\">\"b\"</span><span class=\"p\">]))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+--------------+</span> <span class=\"go\">|coalesce(a, b)|</span> <span class=\"go\">+--------------+</span> <span class=\"go\">|          null|</span> <span class=\"go\">|             1|</span> <span class=\"go\">|             2|</span> <span class=\"go\">+--------------+</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">cDf</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"s1\">'*'</span><span class=\"p\">,</span> <span class=\"n\">coalesce</span><span class=\"p\">(</span><span class=\"n\">cDf</span><span class=\"p\">[</span><span class=\"s2\">\"a\"</span><span class=\"p\">],</span> <span class=\"n\">lit</span><span class=\"p\">(</span><span class=\"mf\">0.0</span><span class=\"p\">)))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+----+----+----------------+</span> <span class=\"go\">|   a|   b|coalesce(a, 0.0)|</span> <span class=\"go\">+----+----+----------------+</span> <span class=\"go\">|null|null|             0.0|</span> <span class=\"go\">|   1|null|             1.0|</span> <span class=\"go\">|null|   2|             0.0|</span> <span class=\"go\">+----+----+----------------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.col\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">col</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.col\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> based on the given column name.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.column\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">column</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.column\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> based on the given column name.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.concat\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">concat</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#concat\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.concat\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Concatenates multiple input columns together into a single column. The function works with strings, binary and compatible array columns.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'abcd'</span><span class=\"p\">,</span><span class=\"s1\">'123'</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,</span> <span class=\"s1\">'d'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">concat</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">d</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='abcd123')]</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">5</span><span class=\"p\">]),</span> <span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">])],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">,</span> <span class=\"s1\">'c'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">concat</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"arr\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.concat_ws\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">concat_ws</code><span class=\"sig-paren\">(</span><em>sep</em>, <em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#concat_ws\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.concat_ws\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Concatenates multiple input string columns together into a single string column, using the given separator.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'abcd'</span><span class=\"p\">,</span><span class=\"s1\">'123'</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,</span> <span class=\"s1\">'d'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">concat_ws</span><span class=\"p\">(</span><span class=\"s1\">'-'</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">d</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='abcd-123')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.conv\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">conv</code><span class=\"sig-paren\">(</span><em>col</em>, <em>fromBase</em>, <em>toBase</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#conv\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.conv\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Convert a number in a string column from one base to another.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s2\">\"010101\"</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'n'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'hex'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(hex='15')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.cos\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">cos</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.cos\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â angle in radians</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\"><p>cosine of the angle, as if computed by <cite>java.lang.Math.cos()</cite>.</p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.cosh\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">cosh</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.cosh\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â hyperbolic angle</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\"><p>hyperbolic cosine of the angle, as if computed by <cite>java.lang.Math.cosh()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.crc32\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">crc32</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#crc32\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.crc32\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Calculates the cyclic redundancy check value  (CRC32) of a binary column and returns the value as a bigint.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ABC'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">crc32</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'crc32'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(crc32=2743272264)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.create_map\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">create_map</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#create_map\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.create_map\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a new map column.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>cols</strong> â list of column names (string) or list of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> expressions that are grouped as key-value pairs, e.g. (key1, value1, key2, value2, â¦).</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">create_map</span><span class=\"p\">(</span><span class=\"s1\">'name'</span><span class=\"p\">,</span> <span class=\"s1\">'age'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"map\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(map={'Alice': 2}), Row(map={'Bob': 5})]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">create_map</span><span class=\"p\">([</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"map\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(map={'Alice': 2}), Row(map={'Bob': 5})]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.0.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.cume_dist\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">cume_dist</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.cume_dist\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.current_date\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">current_date</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#current_date\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.current_date\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the current date as a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DateType</span></code> column.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.current_timestamp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">current_timestamp</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#current_timestamp\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.current_timestamp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the current timestamp as a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">TimestampType</span></code> column.</p> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.date_add\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">date_add</code><span class=\"sig-paren\">(</span><em>start</em>, <em>days</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#date_add\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.date_add\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the date that is <cite>days</cite> days after <cite>start</cite></p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">date_add</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">dt</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'next_date'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(next_date=datetime.date(2015, 4, 9))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.date_format\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">date_format</code><span class=\"sig-paren\">(</span><em>date</em>, <em>format</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#date_format\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.date_format\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.</p> <p>A pattern could be for instance <cite>dd.MM.yyyy</cite> and could return a string like â18.03.1993â. All pattern letters of the Java class <cite>java.text.SimpleDateFormat</cite> can be used.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Use when ever possible specialized functions like <cite>year</cite>. These benefit from a specialized implementation.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">date_format</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">,</span> <span class=\"s1\">'MM/dd/yyy'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'date'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(date='04/08/2015')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.date_sub\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">date_sub</code><span class=\"sig-paren\">(</span><em>start</em>, <em>days</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#date_sub\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.date_sub\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the date that is <cite>days</cite> days before <cite>start</cite></p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">date_sub</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">dt</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'prev_date'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(prev_date=datetime.date(2015, 4, 7))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.date_trunc\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">date_trunc</code><span class=\"sig-paren\">(</span><em>format</em>, <em>timestamp</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#date_trunc\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.date_trunc\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns timestamp truncated to the unit specified by the format.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>format</strong> â âyearâ, âyyyyâ, âyyâ, âmonthâ, âmonâ, âmmâ, âdayâ, âddâ, âhourâ, âminuteâ, âsecondâ, âweekâ, âquarterâ</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28 05:02:11'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'t'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">date_trunc</span><span class=\"p\">(</span><span class=\"s1\">'year'</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'year'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(year=datetime.datetime(1997, 1, 1, 0, 0))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">date_trunc</span><span class=\"p\">(</span><span class=\"s1\">'mon'</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'month'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(month=datetime.datetime(1997, 2, 1, 0, 0))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.datediff\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">datediff</code><span class=\"sig-paren\">(</span><em>end</em>, <em>start</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#datediff\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.datediff\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the number of days from <cite>start</cite> to <cite>end</cite>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,</span><span class=\"s1\">'2015-05-10'</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'d1'</span><span class=\"p\">,</span> <span class=\"s1\">'d2'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">datediff</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">d2</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">d1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'diff'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(diff=32)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.dayofmonth\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">dayofmonth</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#dayofmonth\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.dayofmonth\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the day of the month of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">dayofmonth</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'day'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(day=8)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.dayofweek\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">dayofweek</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#dayofweek\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.dayofweek\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the day of the week of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">dayofweek</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'day'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(day=4)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.dayofyear\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">dayofyear</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#dayofyear\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.dayofyear\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the day of the year of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">dayofyear</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'day'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(day=98)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.decode\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">decode</code><span class=\"sig-paren\">(</span><em>col</em>, <em>charset</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#decode\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.decode\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the first argument into a string from a binary using the provided character set (one of âUS-ASCIIâ, âISO-8859-1â, âUTF-8â, âUTF-16BEâ, âUTF-16LEâ, âUTF-16â).</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.degrees\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">degrees</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.degrees\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts an angle measured in radians to an approximately equivalent angle measured in degrees. :param col: angle in radians :return: angle in degrees, as if computed by <cite>java.lang.Math.toDegrees()</cite></p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.1.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.dense_rank\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">dense_rank</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.dense_rank\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Window function: returns the rank of rows within a window partition, without any gaps.</p> <p>The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.</p> <p>This is equivalent to the DENSE_RANK function in SQL.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.desc\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">desc</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.desc\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a sort expression based on the descending order of the given column name.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.desc_nulls_first\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">desc_nulls_first</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.desc_nulls_first\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.desc_nulls_last\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">desc_nulls_last</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.desc_nulls_last\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.element_at\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">element_at</code><span class=\"sig-paren\">(</span><em>col</em>, <em>extraction</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#element_at\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.element_at\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: Returns element of array at given index in extraction if col is array. Returns value for the given key in extraction if col is map.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â name of column containing array or map</p></li> <li><p><strong>extraction</strong> â index to check for in array or key to check for in map</p></li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The position is not zero based, but 1 based index.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">,</span> <span class=\"s2\">\"c\"</span><span class=\"p\">],),</span> <span class=\"p\">([],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">element_at</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(element_at(data, 1)='a'), Row(element_at(data, 1)=None)]</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([({</span><span class=\"s2\">\"a\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">:</span> <span class=\"mf\">2.0</span><span class=\"p\">},),</span> <span class=\"p\">({},)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">element_at</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"s2\">\"a\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.encode\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">encode</code><span class=\"sig-paren\">(</span><em>col</em>, <em>charset</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#encode\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.encode\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the first argument into a binary from a string using the provided character set (one of âUS-ASCIIâ, âISO-8859-1â, âUTF-8â, âUTF-16BEâ, âUTF-16LEâ, âUTF-16â).</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.exp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">exp</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.exp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the exponential of the given value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.explode\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">explode</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#explode\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.explode\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a new row for each element in the given array or map.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql</span> <span class=\"k\">import</span> <span class=\"n\">Row</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">eDF</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span><span class=\"n\">Row</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">intlist</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"n\">mapfield</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"a\"</span><span class=\"p\">:</span> <span class=\"s2\">\"b\"</span><span class=\"p\">})])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">eDF</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">explode</span><span class=\"p\">(</span><span class=\"n\">eDF</span><span class=\"o\">.</span><span class=\"n\">intlist</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"anInt\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(anInt=1), Row(anInt=2), Row(anInt=3)]</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">eDF</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">explode</span><span class=\"p\">(</span><span class=\"n\">eDF</span><span class=\"o\">.</span><span class=\"n\">mapfield</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+---+-----+</span> <span class=\"go\">|key|value|</span> <span class=\"go\">+---+-----+</span> <span class=\"go\">|  a|    b|</span> <span class=\"go\">+---+-----+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.explode_outer\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">explode_outer</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#explode_outer\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.explode_outer\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a new row for each element in the given array or map. Unlike explode, if the array/map is null or empty then null is produced.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span> <span class=\"gp\">... </span>    <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s2\">\"foo\"</span><span class=\"p\">,</span> <span class=\"s2\">\"bar\"</span><span class=\"p\">],</span> <span class=\"p\">{</span><span class=\"s2\">\"x\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">}),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">[],</span> <span class=\"p\">{}),</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)],</span> <span class=\"gp\">... </span>    <span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"an_array\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a_map\"</span><span class=\"p\">)</span> <span class=\"gp\">... </span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"an_array\"</span><span class=\"p\">,</span> <span class=\"n\">explode_outer</span><span class=\"p\">(</span><span class=\"s2\">\"a_map\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+---+----------+----+-----+</span> <span class=\"go\">| id|  an_array| key|value|</span> <span class=\"go\">+---+----------+----+-----+</span> <span class=\"go\">|  1|[foo, bar]|   x|  1.0|</span> <span class=\"go\">|  2|        []|null| null|</span> <span class=\"go\">|  3|      null|null| null|</span> <span class=\"go\">+---+----------+----+-----+</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a_map\"</span><span class=\"p\">,</span> <span class=\"n\">explode_outer</span><span class=\"p\">(</span><span class=\"s2\">\"an_array\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+---+----------+----+</span> <span class=\"go\">| id|     a_map| col|</span> <span class=\"go\">+---+----------+----+</span> <span class=\"go\">|  1|[x -&gt; 1.0]| foo|</span> <span class=\"go\">|  1|[x -&gt; 1.0]| bar|</span> <span class=\"go\">|  2|        []|null|</span> <span class=\"go\">|  3|      null|null|</span> <span class=\"go\">+---+----------+----+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.expm1\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">expm1</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.expm1\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the exponential of the given value minus one.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.expr\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">expr</code><span class=\"sig-paren\">(</span><em>str</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#expr\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.expr\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Parses the expression string into the column that it represents</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">expr</span><span class=\"p\">(</span><span class=\"s2\">\"length(name)\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(length(name)=5), Row(length(name)=3)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.factorial\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">factorial</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#factorial\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.factorial\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the factorial of the given value.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">5</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'n'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">factorial</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">n</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'f'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(f=120)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.flatten\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">flatten</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#flatten\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.flatten\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: creates a single array from an array of arrays. If a structure of nested arrays is deeper than two levels, only one level of nesting is removed.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">]],),</span> <span class=\"p\">([</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">]],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.floor\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">floor</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.floor\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the floor of the given value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.format_number\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">format_number</code><span class=\"sig-paren\">(</span><em>col</em>, <em>d</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#format_number\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.format_number\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Formats the number X to a format like â#,â#,â#.ââ, rounded to d decimal places with HALF_EVEN round mode, and returns the result as a string.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â the column name of the numeric value to be formatted</p></li> <li><p><strong>d</strong> â the N decimal places</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">5</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">format_number</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'v'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(v='5.0000')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.format_string\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">format_string</code><span class=\"sig-paren\">(</span><em>format</em>, <em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#format_string\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.format_string\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Formats the arguments in printf-style and returns the result as a string column.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â the column name of the numeric value to be formatted</p></li> <li><p><strong>d</strong> â the N decimal places</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"s2\">\"hello\"</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">format_string</span><span class=\"p\">(</span><span class=\"s1\">'</span><span class=\"si\">%d</span><span class=\"s1\"> </span><span class=\"si\">%s</span><span class=\"s1\">'</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">b</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'v'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(v='5 hello')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.from_json\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">from_json</code><span class=\"sig-paren\">(</span><em>col</em>, <em>schema</em>, <em>options={}</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#from_json\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.from_json\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Parses a column containing a JSON string into a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MapType</span></code> with <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StringType</span></code> as keys type, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StructType</span></code> or <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ArrayType</span></code> with the specified schema. Returns <cite>null</cite>, in the case of an unparseable string.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â string column in json format</p></li> <li><p><strong>schema</strong> â a StructType or ArrayType of StructType to use when parsing the json column.</p></li> <li><p><strong>options</strong> â options to control parsing. accepts the same options as the json datasource</p></li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Since Spark 2.3, the DDL-formatted string or a JSON format string is also supported for <code class=\"docutils literal notranslate\"><span class=\"pre\">schema</span></code>.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.types</span> <span class=\"k\">import</span> <span class=\"o\">*</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'''{\"a\": 1}'''</span><span class=\"p\">)]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">StructType</span><span class=\"p\">([</span><span class=\"n\">StructField</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"n\">IntegerType</span><span class=\"p\">())])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">schema</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json=Row(a=1))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"s2\">\"a INT\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json=Row(a=1))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"s2\">\"MAP&lt;STRING,INT&gt;\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json={'a': 1})]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'''[{\"a\": 1}]'''</span><span class=\"p\">)]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">ArrayType</span><span class=\"p\">(</span><span class=\"n\">StructType</span><span class=\"p\">([</span><span class=\"n\">StructField</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"n\">IntegerType</span><span class=\"p\">())]))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">schema</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json=[Row(a=1)])]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">schema_of_json</span><span class=\"p\">(</span><span class=\"n\">lit</span><span class=\"p\">(</span><span class=\"s1\">'''{\"a\": 0}'''</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">schema</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json=Row(a=1))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s1\">'''[1, 2, 3]'''</span><span class=\"p\">)]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">ArrayType</span><span class=\"p\">(</span><span class=\"n\">IntegerType</span><span class=\"p\">())</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">schema</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json=[1, 2, 3])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.1.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.from_unixtime\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">from_unixtime</code><span class=\"sig-paren\">(</span><em>timestamp</em>, <em>format='yyyy-MM-dd HH:mm:ss'</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#from_unixtime\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.from_unixtime\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">conf</span><span class=\"o\">.</span><span class=\"n\">set</span><span class=\"p\">(</span><span class=\"s2\">\"spark.sql.session.timeZone\"</span><span class=\"p\">,</span> <span class=\"s2\">\"America/Los_Angeles\"</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">time_df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">1428476400</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'unix_time'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">time_df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_unixtime</span><span class=\"p\">(</span><span class=\"s1\">'unix_time'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'ts'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(ts='2015-04-08 00:00:00')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">conf</span><span class=\"o\">.</span><span class=\"n\">unset</span><span class=\"p\">(</span><span class=\"s2\">\"spark.sql.session.timeZone\"</span><span class=\"p\">)</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.from_utc_timestamp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">from_utc_timestamp</code><span class=\"sig-paren\">(</span><em>timestamp</em>, <em>tz</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#from_utc_timestamp\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.from_utc_timestamp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and renders that timestamp as a timestamp in the given time zone.</p> <p>However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to the given timezone.</p> <p>This function may return confusing result if the input is a string with timezone, e.g. â2018-03-13T06:18:23+00:00â. The reason is that, Spark firstly cast the string to timestamp according to the timezone in the string, and finally display the result by converting the timestamp to string according to the session local timezone.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>timestamp</strong> â the column that contains timestamps</p></li> <li><p><strong>tz</strong> â a string that has the ID of timezone, e.g. âGMTâ, âAmerica/Los_Angelesâ, etc</p></li> </ul> </dd> </dl> <div class=\"versionchanged\"> <p><span class=\"versionmodified changed\">Changed in version 2.4: </span><cite>tz</cite> can take a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> containing timezone ID strings.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28 10:30:00'</span><span class=\"p\">,</span> <span class=\"s1\">'JST'</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'ts'</span><span class=\"p\">,</span> <span class=\"s1\">'tz'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_utc_timestamp</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">ts</span><span class=\"p\">,</span> <span class=\"s2\">\"PST\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'local_time'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">from_utc_timestamp</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">ts</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">tz</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'local_time'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.get_json_object\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">get_json_object</code><span class=\"sig-paren\">(</span><em>col</em>, <em>path</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#get_json_object\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.get_json_object\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â string column in json format</p></li> <li><p><strong>path</strong> â path to the json object to extract</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span> <span class=\"s1\">'''{\"f1\": \"value1\", \"f2\": \"value2\"}'''</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s2\">\"2\"</span><span class=\"p\">,</span> <span class=\"s1\">'''{\"f1\": \"value12\"}'''</span><span class=\"p\">)]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"jstring\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">get_json_object</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">jstring</span><span class=\"p\">,</span> <span class=\"s1\">'$.f1'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"c0\"</span><span class=\"p\">),</span> \\ <span class=\"gp\">... </span>                  <span class=\"n\">get_json_object</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">jstring</span><span class=\"p\">,</span> <span class=\"s1\">'$.f2'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"c1\"</span><span class=\"p\">)</span> <span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.greatest\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">greatest</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#greatest\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.greatest\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the greatest value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">,</span> <span class=\"s1\">'c'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">greatest</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"greatest\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(greatest=4)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.hash\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">hash</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#hash\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.hash\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Calculates the hash code of given columns, and returns the result as an int column.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ABC'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"nb\">hash</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'hash'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(hash=-757602832)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.0.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.hex\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">hex</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#hex\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.hex\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes hex value of the given column, which could be <a class=\"reference internal\" href=\"#pyspark.sql.types.StringType\" title=\"pyspark.sql.types.StringType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.StringType</span></code></a>, <a class=\"reference internal\" href=\"#pyspark.sql.types.BinaryType\" title=\"pyspark.sql.types.BinaryType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.BinaryType</span></code></a>, <a class=\"reference internal\" href=\"#pyspark.sql.types.IntegerType\" title=\"pyspark.sql.types.IntegerType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.IntegerType</span></code></a> or <a class=\"reference internal\" href=\"#pyspark.sql.types.LongType\" title=\"pyspark.sql.types.LongType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.LongType</span></code></a>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ABC'</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"nb\">hex</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">),</span> <span class=\"nb\">hex</span><span class=\"p\">(</span><span class=\"s1\">'b'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(hex(a)='414243', hex(b)='3')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.hour\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">hour</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#hour\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.hour\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the hours of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08 13:08:15'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'ts'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">hour</span><span class=\"p\">(</span><span class=\"s1\">'ts'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'hour'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(hour=13)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.hypot\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">hypot</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.hypot\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes <code class=\"docutils literal notranslate\"><span class=\"pre\">sqrt(a^2</span> <span class=\"pre\">+</span> <span class=\"pre\">b^2)</span></code> without intermediate overflow or underflow.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.initcap\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">initcap</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#initcap\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.initcap\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Translate the first letter of each word to upper case in the sentence.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ab cd'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">initcap</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'v'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(v='Ab Cd')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.input_file_name\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">input_file_name</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#input_file_name\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.input_file_name\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a string column for the file name of the current Spark task.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.instr\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">instr</code><span class=\"sig-paren\">(</span><em>str</em>, <em>substr</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#instr\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.instr\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'abcd'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">instr</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s=2)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.isnan\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">isnan</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#isnan\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.isnan\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>An expression that returns true iff the column is NaN.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">'nan'</span><span class=\"p\">)),</span> <span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">'nan'</span><span class=\"p\">),</span> <span class=\"mf\">2.0</span><span class=\"p\">)],</span> <span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">isnan</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"r1\"</span><span class=\"p\">),</span> <span class=\"n\">isnan</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">a</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"r2\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r1=False, r2=False), Row(r1=True, r2=True)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.isnull\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">isnull</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#isnull\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.isnull\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>An expression that returns true iff the column is null.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)],</span> <span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">isnull</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"r1\"</span><span class=\"p\">),</span> <span class=\"n\">isnull</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">a</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"r2\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r1=False, r2=False), Row(r1=True, r2=True)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.json_tuple\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">json_tuple</code><span class=\"sig-paren\">(</span><em>col</em>, <em>*fields</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#json_tuple\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.json_tuple\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a new row for a json column according to the given field names.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â string column in json format</p></li> <li><p><strong>fields</strong> â list of fields to extract</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"s2\">\"1\"</span><span class=\"p\">,</span> <span class=\"s1\">'''{\"f1\": \"value1\", \"f2\": \"value2\"}'''</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"s2\">\"2\"</span><span class=\"p\">,</span> <span class=\"s1\">'''{\"f1\": \"value12\"}'''</span><span class=\"p\">)]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"jstring\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">json_tuple</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">jstring</span><span class=\"p\">,</span> <span class=\"s1\">'f1'</span><span class=\"p\">,</span> <span class=\"s1\">'f2'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.lag\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">lag</code><span class=\"sig-paren\">(</span><em>col</em>, <em>count=1</em>, <em>default=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#lag\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.lag\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Window function: returns the value that is <cite>offset</cite> rows before the current row, and <cite>defaultValue</cite> if there is less than <cite>offset</cite> rows before the current row. For example, an <cite>offset</cite> of one will return the previous row at any given point in the window partition.</p> <p>This is equivalent to the LAG function in SQL.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â name of column or expression</p></li> <li><p><strong>count</strong> â number of row to extend</p></li> <li><p><strong>default</strong> â default value</p></li> </ul> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.last_day\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">last_day</code><span class=\"sig-paren\">(</span><em>date</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#last_day\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.last_day\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the last day of the month which the given date belongs to.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-10'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'d'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">last_day</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">d</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'date'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(date=datetime.date(1997, 2, 28))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.lead\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">lead</code><span class=\"sig-paren\">(</span><em>col</em>, <em>count=1</em>, <em>default=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#lead\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.lead\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Window function: returns the value that is <cite>offset</cite> rows after the current row, and <cite>defaultValue</cite> if there is less than <cite>offset</cite> rows after the current row. For example, an <cite>offset</cite> of one will return the next row at any given point in the window partition.</p> <p>This is equivalent to the LEAD function in SQL.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â name of column or expression</p></li> <li><p><strong>count</strong> â number of row to extend</p></li> <li><p><strong>default</strong> â default value</p></li> </ul> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.least\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">least</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#least\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.least\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the least value of the list of column names, skipping null values. This function takes at least 2 parameters. It will return null iff all parameters are null.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">,</span> <span class=\"s1\">'c'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">least</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">c</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"least\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(least=1)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.length\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">length</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#length\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.length\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the character length of string data or number of bytes of binary data. The length of character data includes the trailing spaces. The length of binary data includes binary zeros.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ABC '</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'length'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(length=4)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.levenshtein\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">levenshtein</code><span class=\"sig-paren\">(</span><em>left</em>, <em>right</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#levenshtein\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.levenshtein\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the Levenshtein distance of the two given strings.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df0</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'kitten'</span><span class=\"p\">,</span> <span class=\"s1\">'sitting'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'l'</span><span class=\"p\">,</span> <span class=\"s1\">'r'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df0</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">levenshtein</span><span class=\"p\">(</span><span class=\"s1\">'l'</span><span class=\"p\">,</span> <span class=\"s1\">'r'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'d'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(d=3)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.lit\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">lit</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.lit\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> of literal value.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">lit</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'height'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s1\">'spark_user'</span><span class=\"p\">,</span> <span class=\"n\">lit</span><span class=\"p\">(</span><span class=\"kc\">True</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">take</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"go\">[Row(height=5, spark_user=True)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.locate\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">locate</code><span class=\"sig-paren\">(</span><em>substr</em>, <em>str</em>, <em>pos=1</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#locate\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.locate\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Locate the position of the first occurrence of substr in a string column, after position pos.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The position is not zero based, but 1 based index. Returns 0 if substr could not be found in str.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>substr</strong> â a string</p></li> <li><p><strong>str</strong> â a Column of <a class=\"reference internal\" href=\"#pyspark.sql.types.StringType\" title=\"pyspark.sql.types.StringType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.StringType</span></code></a></p></li> <li><p><strong>pos</strong> â start position (zero based)</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'abcd'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">locate</span><span class=\"p\">(</span><span class=\"s1\">'b'</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s=2)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.log\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">log</code><span class=\"sig-paren\">(</span><em>arg1</em>, <em>arg2=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#log\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.log\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the first argument-based logarithm of the second argument.</p> <p>If there is only one argument, then this takes the natural logarithm of the argument.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mf\">10.0</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'ten'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">l</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">.</span><span class=\"n\">ten</span><span class=\"p\">)[:</span><span class=\"mi\">7</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">['0.30102', '0.69897']</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'e'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">l</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">.</span><span class=\"n\">e</span><span class=\"p\">)[:</span><span class=\"mi\">7</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">['0.69314', '1.60943']</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.log10\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">log10</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.log10\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the logarithm of the given value in Base 10.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.log1p\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">log1p</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.log1p\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the natural logarithm of the given value plus one.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.log2\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">log2</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#log2\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.log2\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the base-2 logarithm of the argument.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">4</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">log2</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'log2'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(log2=2.0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.lower\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">lower</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.lower\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts a string column to lower case.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.lpad\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">lpad</code><span class=\"sig-paren\">(</span><em>col</em>, <em>len</em>, <em>pad</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#lpad\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.lpad\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Left-pad the string column to width <cite>len</cite> with <cite>pad</cite>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'abcd'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">lpad</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s1\">'#'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='##abcd')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.ltrim\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">ltrim</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.ltrim\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Trim the spaces from left end for the specified string value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.map_concat\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">map_concat</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#map_concat\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.map_concat\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the union of all the given maps.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>cols</strong> â list of column names (string) or list of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> expressions</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">map_concat</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span><span class=\"s2\">\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c', 1, 'd') as map2\"</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">map_concat</span><span class=\"p\">(</span><span class=\"s2\">\"map1\"</span><span class=\"p\">,</span> <span class=\"s2\">\"map2\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"map3\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">(</span><span class=\"n\">truncate</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span> <span class=\"go\">+--------------------------------+</span> <span class=\"go\">|map3                            |</span> <span class=\"go\">+--------------------------------+</span> <span class=\"go\">|[1 -&gt; a, 2 -&gt; b, 3 -&gt; c, 1 -&gt; d]|</span> <span class=\"go\">+--------------------------------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.map_from_arrays\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">map_from_arrays</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#map_from_arrays\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.map_from_arrays\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a new map from two arrays.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col1</strong> â name of column containing a set of keys. All elements should not be null</p></li> <li><p><strong>col2</strong> â name of column containing a set of values</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s1\">'b'</span><span class=\"p\">])],</span> <span class=\"p\">[</span><span class=\"s1\">'k'</span><span class=\"p\">,</span> <span class=\"s1\">'v'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">map_from_arrays</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">v</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"map\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+----------------+</span> <span class=\"go\">|             map|</span> <span class=\"go\">+----------------+</span> <span class=\"go\">|[2 -&gt; a, 5 -&gt; b]|</span> <span class=\"go\">+----------------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.map_from_entries\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">map_from_entries</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#map_from_entries\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.map_from_entries\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: Returns a map created from the given array of entries.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">map_from_entries</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span><span class=\"s2\">\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\"</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">map_from_entries</span><span class=\"p\">(</span><span class=\"s2\">\"data\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"map\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+----------------+</span> <span class=\"go\">|             map|</span> <span class=\"go\">+----------------+</span> <span class=\"go\">|[1 -&gt; a, 2 -&gt; b]|</span> <span class=\"go\">+----------------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.map_keys\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">map_keys</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#map_keys\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.map_keys\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: Returns an unordered array containing the keys of the map.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">map_keys</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span><span class=\"s2\">\"SELECT map(1, 'a', 2, 'b') as data\"</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">map_keys</span><span class=\"p\">(</span><span class=\"s2\">\"data\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"keys\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+------+</span> <span class=\"go\">|  keys|</span> <span class=\"go\">+------+</span> <span class=\"go\">|[1, 2]|</span> <span class=\"go\">+------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.map_values\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">map_values</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#map_values\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.map_values\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: Returns an unordered array containing the values of the map.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">map_values</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"p\">(</span><span class=\"s2\">\"SELECT map(1, 'a', 2, 'b') as data\"</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">map_values</span><span class=\"p\">(</span><span class=\"s2\">\"data\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"values\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+------+</span> <span class=\"go\">|values|</span> <span class=\"go\">+------+</span> <span class=\"go\">|[a, b]|</span> <span class=\"go\">+------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.md5\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">md5</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#md5\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.md5\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Calculates the MD5 digest and returns the value as a 32 character hex string.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ABC'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">md5</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'hash'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.minute\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">minute</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#minute\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.minute\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the minutes of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08 13:08:15'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'ts'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">minute</span><span class=\"p\">(</span><span class=\"s1\">'ts'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'minute'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(minute=8)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.monotonically_increasing_id\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">monotonically_increasing_id</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#monotonically_increasing_id\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.monotonically_increasing_id\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>A column that generates monotonically increasing 64-bit integers.</p> <p>The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The function is non-deterministic because its result depends on partition IDs.</p> </div> <p>As an example, consider a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DataFrame</span></code> with two partitions, each with 3 records. This expression would return the following IDs: 0, 1, 2, 8589934592 (1L &lt;&lt; 33), 8589934593, 8589934594.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df0</span> <span class=\"o\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mapPartitions</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,),</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,)])</span><span class=\"o\">.</span><span class=\"n\">toDF</span><span class=\"p\">([</span><span class=\"s1\">'col1'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df0</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">monotonically_increasing_id</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'id'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.month\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">month</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#month\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.month\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><blockquote> <div><p>Extract the month of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">month</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'month'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(month=4)]</span> </pre></div> </div> </div></blockquote> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.months_between\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">months_between</code><span class=\"sig-paren\">(</span><em>date1</em>, <em>date2</em>, <em>roundOff=True</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#months_between\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.months_between\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns number of months between dates date1 and date2. If date1 is later than date2, then the result is positive. If date1 and date2 are on the same day of month, or both are the last day of month, returns an integer (time of day will be ignored). The result is rounded off to 8 digits unless <cite>roundOff</cite> is set to <cite>False</cite>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28 10:30:00'</span><span class=\"p\">,</span> <span class=\"s1\">'1996-10-30'</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'date1'</span><span class=\"p\">,</span> <span class=\"s1\">'date2'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">months_between</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">date1</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">date2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'months'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(months=3.94959677)]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">months_between</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">date1</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">date2</span><span class=\"p\">,</span> <span class=\"kc\">False</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'months'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(months=3.9495967741935485)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.nanvl\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">nanvl</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#nanvl\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.nanvl\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns col1 if it is not NaN, or col2 if col1 is NaN.</p> <p>Both inputs should be floating point columns (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DoubleType</span></code> or <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">FloatType</span></code>).</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">'nan'</span><span class=\"p\">)),</span> <span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"s1\">'nan'</span><span class=\"p\">),</span> <span class=\"mf\">2.0</span><span class=\"p\">)],</span> <span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">nanvl</span><span class=\"p\">(</span><span class=\"s2\">\"a\"</span><span class=\"p\">,</span> <span class=\"s2\">\"b\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"r1\"</span><span class=\"p\">),</span> <span class=\"n\">nanvl</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">b</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"r2\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.next_day\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">next_day</code><span class=\"sig-paren\">(</span><em>date</em>, <em>dayOfWeek</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#next_day\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.next_day\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the first date which is later than the value of the date column.</p> <dl class=\"simple\"> <dt>Day of the week parameter is case insensitive, and accepts:</dt><dd><p>âMonâ, âTueâ, âWedâ, âThuâ, âFriâ, âSatâ, âSunâ.</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-07-27'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'d'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">next_day</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"s1\">'Sun'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'date'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(date=datetime.date(2015, 8, 2))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.ntile\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">ntile</code><span class=\"sig-paren\">(</span><em>n</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#ntile\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.ntile\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Window function: returns the ntile group id (from 1 to <cite>n</cite> inclusive) in an ordered window partition. For example, if <cite>n</cite> is 4, the first quarter of the rows will get value 1, the second quarter will get 2, the third quarter will get 3, and the last quarter will get 4.</p> <p>This is equivalent to the NTILE function in SQL.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>n</strong> â an integer</p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.pandas_udf\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">pandas_udf</code><span class=\"sig-paren\">(</span><em>f=None</em>, <em>returnType=None</em>, <em>functionType=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#pandas_udf\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.pandas_udf\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a vectorized user defined function (UDF).</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>f</strong> â user-defined function. A python function if used as a standalone function</p></li> <li><p><strong>returnType</strong> â the return type of the user-defined function. The value can be either a <a class=\"reference internal\" href=\"#pyspark.sql.types.DataType\" title=\"pyspark.sql.types.DataType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.DataType</span></code></a> object or a DDL-formatted type string.</p></li> <li><p><strong>functionType</strong> â an enum value in <a class=\"reference internal\" href=\"#pyspark.sql.functions.PandasUDFType\" title=\"pyspark.sql.functions.PandasUDFType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.functions.PandasUDFType</span></code></a>. Default: SCALAR.</p></li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Experimental</p> </div> <p>The function type of the UDF can be one of the following:</p> <ol class=\"arabic\"> <li><p>SCALAR</p> <p>A scalar UDF defines a transformation: One or more <cite>pandas.Series</cite> -&gt; A <cite>pandas.Series</cite>. The length of the returned <cite>pandas.Series</cite> must be of the same as the input <cite>pandas.Series</cite>.</p> <p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MapType</span></code>, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StructType</span></code> are currently not supported as output types.</p> <p>Scalar UDFs are used with <a class=\"reference internal\" href=\"#pyspark.sql.DataFrame.withColumn\" title=\"pyspark.sql.DataFrame.withColumn\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">pyspark.sql.DataFrame.withColumn()</span></code></a> and <a class=\"reference internal\" href=\"#pyspark.sql.DataFrame.select\" title=\"pyspark.sql.DataFrame.select\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">pyspark.sql.DataFrame.select()</span></code></a>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">pandas_udf</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.types</span> <span class=\"k\">import</span> <span class=\"n\">IntegerType</span><span class=\"p\">,</span> <span class=\"n\">StringType</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">slen</span> <span class=\"o\">=</span> <span class=\"n\">pandas_udf</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">s</span><span class=\"p\">:</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">len</span><span class=\"p\">(),</span> <span class=\"n\">IntegerType</span><span class=\"p\">())</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">pandas_udf</span><span class=\"p\">(</span><span class=\"n\">StringType</span><span class=\"p\">())</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">to_upper</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"k\">return</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">upper</span><span class=\"p\">()</span> <span class=\"gp\">...</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">pandas_udf</span><span class=\"p\">(</span><span class=\"s2\">\"integer\"</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span><span class=\"o\">.</span><span class=\"n\">SCALAR</span><span class=\"p\">)</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">add_one</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"gp\">...</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s2\">\"John Doe\"</span><span class=\"p\">,</span> <span class=\"mi\">21</span><span class=\"p\">)],</span> <span class=\"gp\">... </span>                           <span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"name\"</span><span class=\"p\">,</span> <span class=\"s2\">\"age\"</span><span class=\"p\">))</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">slen</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"slen(name)\"</span><span class=\"p\">),</span> <span class=\"n\">to_upper</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">),</span> <span class=\"n\">add_one</span><span class=\"p\">(</span><span class=\"s2\">\"age\"</span><span class=\"p\">))</span> \\ <span class=\"gp\">... </span>    <span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"go\">+----------+--------------+------------+</span> <span class=\"go\">|slen(name)|to_upper(name)|add_one(age)|</span> <span class=\"go\">+----------+--------------+------------+</span> <span class=\"go\">|         8|      JOHN DOE|          22|</span> <span class=\"go\">+----------+--------------+------------+</span> </pre></div> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The length of <cite>pandas.Series</cite> within a scalar UDF is not that of the whole input column, but is the length of an internal batch used for each call to the function. Therefore, this can be used, for example, to ensure the length of each returned <cite>pandas.Series</cite>, and can not be used as the column length.</p> </div> </li> <li><p>GROUPED_MAP</p> <p>A grouped map UDF defines transformation: A <cite>pandas.DataFrame</cite> -&gt; A <cite>pandas.DataFrame</cite> The returnType should be a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StructType</span></code> describing the schema of the returned <cite>pandas.DataFrame</cite>. The column labels of the returned <cite>pandas.DataFrame</cite> must either match the field names in the defined returnType schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices. The length of the returned <cite>pandas.DataFrame</cite> can be arbitrary.</p> <p>Grouped map UDFs are used with <a class=\"reference internal\" href=\"#pyspark.sql.GroupedData.apply\" title=\"pyspark.sql.GroupedData.apply\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">pyspark.sql.GroupedData.apply()</span></code></a>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">pandas_udf</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span> <span class=\"gp\">... </span>    <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">3.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">5.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">10.0</span><span class=\"p\">)],</span> <span class=\"gp\">... </span>    <span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"v\"</span><span class=\"p\">))</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">pandas_udf</span><span class=\"p\">(</span><span class=\"s2\">\"id long, v double\"</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span><span class=\"o\">.</span><span class=\"n\">GROUPED_MAP</span><span class=\"p\">)</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">normalize</span><span class=\"p\">(</span><span class=\"n\">pdf</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">pdf</span><span class=\"o\">.</span><span class=\"n\">v</span> <span class=\"gp\">... </span>    <span class=\"k\">return</span> <span class=\"n\">pdf</span><span class=\"o\">.</span><span class=\"n\">assign</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">v</span> <span class=\"o\">-</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">())</span> <span class=\"o\">/</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">std</span><span class=\"p\">())</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">normalize</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"go\">+---+-------------------+</span> <span class=\"go\">| id|                  v|</span> <span class=\"go\">+---+-------------------+</span> <span class=\"go\">|  1|-0.7071067811865475|</span> <span class=\"go\">|  1| 0.7071067811865475|</span> <span class=\"go\">|  2|-0.8320502943378437|</span> <span class=\"go\">|  2|-0.2773500981126146|</span> <span class=\"go\">|  2| 1.1094003924504583|</span> <span class=\"go\">+---+-------------------+</span> </pre></div> </div> <p>Alternatively, the user can define a function that takes two arguments. In this case, the grouping key(s) will be passed as the first argument and the data will be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy data types, e.g., <cite>numpy.int32</cite> and <cite>numpy.float64</cite>. The data will still be passed in as a <cite>pandas.DataFrame</cite> containing all columns from the original Spark DataFrame. This is useful when the user does not want to hardcode grouping key(s) in the function.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">pandas_udf</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span> <span class=\"gp\">... </span>    <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">3.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">5.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">10.0</span><span class=\"p\">)],</span> <span class=\"gp\">... </span>    <span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"v\"</span><span class=\"p\">))</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">pandas_udf</span><span class=\"p\">(</span><span class=\"s2\">\"id long, v double\"</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span><span class=\"o\">.</span><span class=\"n\">GROUPED_MAP</span><span class=\"p\">)</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">mean_udf</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">pdf</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"c1\"># key is a tuple of one numpy.int64, which is the value</span> <span class=\"gp\">... </span>    <span class=\"c1\"># of 'id' for the current group</span> <span class=\"gp\">... </span>    <span class=\"k\">return</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">([</span><span class=\"n\">key</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pdf</span><span class=\"o\">.</span><span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(),)])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s1\">'id'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">mean_udf</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"go\">+---+---+</span> <span class=\"go\">| id|  v|</span> <span class=\"go\">+---+---+</span> <span class=\"go\">|  1|1.5|</span> <span class=\"go\">|  2|6.0|</span> <span class=\"go\">+---+---+</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">pandas_udf</span><span class=\"p\">(</span> <span class=\"gp\">... </span>   <span class=\"s2\">\"id long, `ceil(v / 2)` long, v double\"</span><span class=\"p\">,</span> <span class=\"gp\">... </span>   <span class=\"n\">PandasUDFType</span><span class=\"o\">.</span><span class=\"n\">GROUPED_MAP</span><span class=\"p\">)</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"k\">def</span> <span class=\"nf\">sum_udf</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">pdf</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"c1\"># key is a tuple of two numpy.int64s, which is the values</span> <span class=\"gp\">... </span>    <span class=\"c1\"># of 'id' and 'ceil(df.v / 2)' for the current group</span> <span class=\"gp\">... </span>    <span class=\"k\">return</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">([</span><span class=\"n\">key</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">pdf</span><span class=\"o\">.</span><span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(),)])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">ceil</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">v</span> <span class=\"o\">/</span> <span class=\"mi\">2</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">apply</span><span class=\"p\">(</span><span class=\"n\">sum_udf</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"go\">+---+-----------+----+</span> <span class=\"go\">| id|ceil(v / 2)|   v|</span> <span class=\"go\">+---+-----------+----+</span> <span class=\"go\">|  2|          5|10.0|</span> <span class=\"go\">|  1|          1| 3.0|</span> <span class=\"go\">|  2|          3| 5.0|</span> <span class=\"go\">|  2|          2| 3.0|</span> <span class=\"go\">+---+-----------+----+</span> </pre></div> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If returning a new <cite>pandas.DataFrame</cite> constructed with a dictionary, it is recommended to explicitly index the columns by name to ensure the positions are correct, or alternatively use an <cite>OrderedDict</cite>. For example, <cite>pd.DataFrame({âidâ: ids, âaâ: data}, columns=[âidâ, âaâ])</cite> or <cite>pd.DataFrame(OrderedDict([(âidâ, ids), (âaâ, data)]))</cite>.</p> </div> <div class=\"admonition seealso\"> <p class=\"admonition-title\">See also</p> <p><a class=\"reference internal\" href=\"#pyspark.sql.GroupedData.apply\" title=\"pyspark.sql.GroupedData.apply\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">pyspark.sql.GroupedData.apply()</span></code></a></p> </div> </li> <li><p>GROUPED_AGG</p> <p>A grouped aggregate UDF defines a transformation: One or more <cite>pandas.Series</cite> -&gt; A scalar The <cite>returnType</cite> should be a primitive data type, e.g., <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DoubleType</span></code>. The returned scalar can be either a python primitive type, e.g., <cite>int</cite> or <cite>float</cite> or a numpy data type, e.g., <cite>numpy.int64</cite> or <cite>numpy.float64</cite>.</p> <p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MapType</span></code> and <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StructType</span></code> are currently not supported as output types.</p> <p>Group aggregate UDFs are used with <a class=\"reference internal\" href=\"#pyspark.sql.GroupedData.agg\" title=\"pyspark.sql.GroupedData.agg\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">pyspark.sql.GroupedData.agg()</span></code></a> and <a class=\"reference internal\" href=\"#pyspark.sql.Window\" title=\"pyspark.sql.Window\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.Window</span></code></a></p> <p>This example shows using grouped aggregated UDFs with groupby:</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">pandas_udf</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span> <span class=\"gp\">... </span>    <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">3.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">5.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">10.0</span><span class=\"p\">)],</span> <span class=\"gp\">... </span>    <span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"v\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">pandas_udf</span><span class=\"p\">(</span><span class=\"s2\">\"double\"</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span><span class=\"o\">.</span><span class=\"n\">GROUPED_AGG</span><span class=\"p\">)</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">mean_udf</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"k\">return</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"n\">mean_udf</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'v'</span><span class=\"p\">]))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"go\">+---+-----------+</span> <span class=\"go\">| id|mean_udf(v)|</span> <span class=\"go\">+---+-----------+</span> <span class=\"go\">|  1|        1.5|</span> <span class=\"go\">|  2|        6.0|</span> <span class=\"go\">+---+-----------+</span> </pre></div> </div> <p>This example shows using grouped aggregated UDFs as window functions. Note that only unbounded window frame is supported at the moment:</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.functions</span> <span class=\"k\">import</span> <span class=\"n\">pandas_udf</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql</span> <span class=\"k\">import</span> <span class=\"n\">Window</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span> <span class=\"gp\">... </span>    <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mf\">2.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">3.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">5.0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mf\">10.0</span><span class=\"p\">)],</span> <span class=\"gp\">... </span>    <span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"v\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">pandas_udf</span><span class=\"p\">(</span><span class=\"s2\">\"double\"</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span><span class=\"o\">.</span><span class=\"n\">GROUPED_AGG</span><span class=\"p\">)</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">mean_udf</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"k\">return</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">Window</span> \\ <span class=\"gp\">... </span>    <span class=\"o\">.</span><span class=\"n\">partitionBy</span><span class=\"p\">(</span><span class=\"s1\">'id'</span><span class=\"p\">)</span> \\ <span class=\"gp\">... </span>    <span class=\"o\">.</span><span class=\"n\">rowsBetween</span><span class=\"p\">(</span><span class=\"n\">Window</span><span class=\"o\">.</span><span class=\"n\">unboundedPreceding</span><span class=\"p\">,</span> <span class=\"n\">Window</span><span class=\"o\">.</span><span class=\"n\">unboundedFollowing</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s1\">'mean_v'</span><span class=\"p\">,</span> <span class=\"n\">mean_udf</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'v'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">over</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"go\">+---+----+------+</span> <span class=\"go\">| id|   v|mean_v|</span> <span class=\"go\">+---+----+------+</span> <span class=\"go\">|  1| 1.0|   1.5|</span> <span class=\"go\">|  1| 2.0|   1.5|</span> <span class=\"go\">|  2| 3.0|   6.0|</span> <span class=\"go\">|  2| 5.0|   6.0|</span> <span class=\"go\">|  2|10.0|   6.0|</span> <span class=\"go\">+---+----+------+</span> </pre></div> </div> <div class=\"admonition seealso\"> <p class=\"admonition-title\">See also</p> <p><a class=\"reference internal\" href=\"#pyspark.sql.GroupedData.agg\" title=\"pyspark.sql.GroupedData.agg\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">pyspark.sql.GroupedData.agg()</span></code></a> and <a class=\"reference internal\" href=\"#pyspark.sql.Window\" title=\"pyspark.sql.Window\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.Window</span></code></a></p> </div> </li> </ol> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The user-defined functions are considered deterministic by default. Due to optimization, duplicate invocations may be eliminated or the function may even be invoked more times than it is present in the query. If your function is not deterministic, call <cite>asNondeterministic</cite> on the user defined function. E.g.:</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">pandas_udf</span><span class=\"p\">(</span><span class=\"s1\">'double'</span><span class=\"p\">,</span> <span class=\"n\">PandasUDFType</span><span class=\"o\">.</span><span class=\"n\">SCALAR</span><span class=\"p\">)</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">random</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span> <span class=\"gp\">... </span>    <span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span> <span class=\"gp\">... </span>    <span class=\"k\">return</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">random</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">asNondeterministic</span><span class=\"p\">()</span>  <span class=\"c1\"># doctest: +SKIP</span> </pre></div> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The user-defined functions do not support conditional expressions or short circuiting in boolean expressions and it ends up with being executed all internally. If the functions can fail on special rows, the workaround is to incorporate the condition into the functions.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The user-defined functions do not take keyword arguments on the calling side.</p> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.percent_rank\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">percent_rank</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.percent_rank\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Window function: returns the relative rank (i.e. percentile) of rows within a window partition.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.posexplode\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">posexplode</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#posexplode\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.posexplode\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a new row for each element with position in the given array or map.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql</span> <span class=\"k\">import</span> <span class=\"n\">Row</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">eDF</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([</span><span class=\"n\">Row</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">intlist</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"n\">mapfield</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">\"a\"</span><span class=\"p\">:</span> <span class=\"s2\">\"b\"</span><span class=\"p\">})])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">eDF</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">posexplode</span><span class=\"p\">(</span><span class=\"n\">eDF</span><span class=\"o\">.</span><span class=\"n\">intlist</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">eDF</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">posexplode</span><span class=\"p\">(</span><span class=\"n\">eDF</span><span class=\"o\">.</span><span class=\"n\">mapfield</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+---+---+-----+</span> <span class=\"go\">|pos|key|value|</span> <span class=\"go\">+---+---+-----+</span> <span class=\"go\">|  0|  a|    b|</span> <span class=\"go\">+---+---+-----+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.1.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.posexplode_outer\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">posexplode_outer</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#posexplode_outer\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.posexplode_outer\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns a new row for each element with position in the given array or map. Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span> <span class=\"gp\">... </span>    <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s2\">\"foo\"</span><span class=\"p\">,</span> <span class=\"s2\">\"bar\"</span><span class=\"p\">],</span> <span class=\"p\">{</span><span class=\"s2\">\"x\"</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">}),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">[],</span> <span class=\"p\">{}),</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)],</span> <span class=\"gp\">... </span>    <span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"an_array\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a_map\"</span><span class=\"p\">)</span> <span class=\"gp\">... </span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"an_array\"</span><span class=\"p\">,</span> <span class=\"n\">posexplode_outer</span><span class=\"p\">(</span><span class=\"s2\">\"a_map\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+---+----------+----+----+-----+</span> <span class=\"go\">| id|  an_array| pos| key|value|</span> <span class=\"go\">+---+----------+----+----+-----+</span> <span class=\"go\">|  1|[foo, bar]|   0|   x|  1.0|</span> <span class=\"go\">|  2|        []|null|null| null|</span> <span class=\"go\">|  3|      null|null|null| null|</span> <span class=\"go\">+---+----------+----+----+-----+</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"a_map\"</span><span class=\"p\">,</span> <span class=\"n\">posexplode_outer</span><span class=\"p\">(</span><span class=\"s2\">\"an_array\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+---+----------+----+----+</span> <span class=\"go\">| id|     a_map| pos| col|</span> <span class=\"go\">+---+----------+----+----+</span> <span class=\"go\">|  1|[x -&gt; 1.0]|   0| foo|</span> <span class=\"go\">|  1|[x -&gt; 1.0]|   1| bar|</span> <span class=\"go\">|  2|        []|null|null|</span> <span class=\"go\">|  3|      null|null|null|</span> <span class=\"go\">+---+----------+----+----+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.pow\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">pow</code><span class=\"sig-paren\">(</span><em>col1</em>, <em>col2</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.pow\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the value of the first argument raised to the power of the second argument.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.quarter\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">quarter</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#quarter\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.quarter\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the quarter of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">quarter</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'quarter'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(quarter=2)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.radians\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">radians</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.radians\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts an angle measured in degrees to an approximately equivalent angle measured in radians. :param col: angle in degrees :return: angle in radians, as if computed by <cite>java.lang.Math.toRadians()</cite></p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.1.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.rand\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">rand</code><span class=\"sig-paren\">(</span><em>seed=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#rand\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.rand\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Generates a random column with independent and identically distributed (i.i.d.) samples from U[0.0, 1.0].</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The function is non-deterministic in general case.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s1\">'rand'</span><span class=\"p\">,</span> <span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(age=2, name='Alice', rand=1.1568609015300986),</span> <span class=\"go\"> Row(age=5, name='Bob', rand=1.403379671529166)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.randn\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">randn</code><span class=\"sig-paren\">(</span><em>seed=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#randn\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.randn\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Generates a column with independent and identically distributed (i.i.d.) samples from the standard normal distribution.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The function is non-deterministic in general case.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"p\">(</span><span class=\"s1\">'randn'</span><span class=\"p\">,</span> <span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">42</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(age=2, name='Alice', randn=-0.7556247885860078),</span> <span class=\"go\">Row(age=5, name='Bob', randn=-0.0861619008451133)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.rank\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">rank</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.rank\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Window function: returns the rank of rows within a window partition.</p> <p>The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.</p> <p>This is equivalent to the RANK function in SQL.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.regexp_extract\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">regexp_extract</code><span class=\"sig-paren\">(</span><em>str</em>, <em>pattern</em>, <em>idx</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#regexp_extract\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.regexp_extract\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract a specific group matched by a Java regex, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'100-200'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'str'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">regexp_extract</span><span class=\"p\">(</span><span class=\"s1\">'str'</span><span class=\"p\">,</span> <span class=\"sa\">r</span><span class=\"s1\">'(\\d+)-(\\d+)'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'d'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(d='100')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'foo'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'str'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">regexp_extract</span><span class=\"p\">(</span><span class=\"s1\">'str'</span><span class=\"p\">,</span> <span class=\"sa\">r</span><span class=\"s1\">'(\\d+)'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'d'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(d='')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'aaaac'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'str'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">regexp_extract</span><span class=\"p\">(</span><span class=\"s1\">'str'</span><span class=\"p\">,</span> <span class=\"s1\">'(a+)(b)?(c)'</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'d'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(d='')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.regexp_replace\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">regexp_replace</code><span class=\"sig-paren\">(</span><em>str</em>, <em>pattern</em>, <em>replacement</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#regexp_replace\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.regexp_replace\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Replace all substrings of the specified string value that match regexp with rep.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'100-200'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'str'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">regexp_replace</span><span class=\"p\">(</span><span class=\"s1\">'str'</span><span class=\"p\">,</span> <span class=\"sa\">r</span><span class=\"s1\">'(\\d+)'</span><span class=\"p\">,</span> <span class=\"s1\">'--'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'d'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(d='-----')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.repeat\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">repeat</code><span class=\"sig-paren\">(</span><em>col</em>, <em>n</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#repeat\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.repeat\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Repeats a string column n times, and returns it as a new string column.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ab'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">repeat</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='ababab')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.reverse\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">reverse</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#reverse\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.reverse\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns a reversed string or an array with reverse order of elements.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'Spark SQL'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">reverse</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='LQS krapS')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],)</span> <span class=\"p\">,([</span><span class=\"mi\">1</span><span class=\"p\">],)</span> <span class=\"p\">,([],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">reverse</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.rint\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">rint</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.rint\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the double value that is closest in value to the argument and is equal to a mathematical integer.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.round\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">round</code><span class=\"sig-paren\">(</span><em>col</em>, <em>scale=0</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#round\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.round\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Round the given value to <cite>scale</cite> decimal places using HALF_UP rounding mode if <cite>scale</cite> &gt;= 0 or at integral part when <cite>scale</cite> &lt; 0.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mf\">2.5</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"nb\">round</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=3.0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.row_number\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">row_number</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.row_number\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Window function: returns a sequential number starting at 1 within a window partition.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.rpad\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">rpad</code><span class=\"sig-paren\">(</span><em>col</em>, <em>len</em>, <em>pad</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#rpad\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.rpad\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Right-pad the string column to width <cite>len</cite> with <cite>pad</cite>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'abcd'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">rpad</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"s1\">'#'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='abcd##')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.rtrim\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">rtrim</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.rtrim\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Trim the spaces from right end for the specified string value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.schema_of_json\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">schema_of_json</code><span class=\"sig-paren\">(</span><em>json</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#schema_of_json\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.schema_of_json\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Parses a JSON string and infers its schema in DDL format.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>json</strong> â a JSON string or a string literal containing a JSON string.</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">schema_of_json</span><span class=\"p\">(</span><span class=\"s1\">'{\"a\": 0}'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json='struct&lt;a:bigint&gt;')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.second\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">second</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#second\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.second\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the seconds of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08 13:08:15'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'ts'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">second</span><span class=\"p\">(</span><span class=\"s1\">'ts'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'second'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(second=15)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sequence\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sequence</code><span class=\"sig-paren\">(</span><em>start</em>, <em>stop</em>, <em>step=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#sequence\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.sequence\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Generate a sequence of integers from <cite>start</cite> to <cite>stop</cite>, incrementing by <cite>step</cite>. If <cite>step</cite> is not set, incrementing by 1 if <cite>start</cite> is less than or equal to <cite>stop</cite>, otherwise -1.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df1</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)],</span> <span class=\"p\">(</span><span class=\"s1\">'C1'</span><span class=\"p\">,</span> <span class=\"s1\">'C2'</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df1</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">sequence</span><span class=\"p\">(</span><span class=\"s1\">'C1'</span><span class=\"p\">,</span> <span class=\"s1\">'C2'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=[-2, -1, 0, 1, 2])]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df2</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">)],</span> <span class=\"p\">(</span><span class=\"s1\">'C1'</span><span class=\"p\">,</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"s1\">'C3'</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df2</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">sequence</span><span class=\"p\">(</span><span class=\"s1\">'C1'</span><span class=\"p\">,</span> <span class=\"s1\">'C2'</span><span class=\"p\">,</span> <span class=\"s1\">'C3'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=[4, 2, 0, -2, -4])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sha1\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sha1</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#sha1\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.sha1\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the hex string result of SHA-1.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ABC'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">sha1</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'hash'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sha2\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sha2</code><span class=\"sig-paren\">(</span><em>col</em>, <em>numBits</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#sha2\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.sha2\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512). The numBits indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256).</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">digests</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">sha2</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">digests</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"go\">Row(s='3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043')</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">digests</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"go\">Row(s='cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961')</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.shiftLeft\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">shiftLeft</code><span class=\"sig-paren\">(</span><em>col</em>, <em>numBits</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#shiftLeft\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.shiftLeft\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Shift the given value numBits left.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">21</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">shiftLeft</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=42)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.shiftRight\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">shiftRight</code><span class=\"sig-paren\">(</span><em>col</em>, <em>numBits</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#shiftRight\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.shiftRight\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>(Signed) shift the given value numBits right.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">42</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">shiftRight</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=21)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.shiftRightUnsigned\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">shiftRightUnsigned</code><span class=\"sig-paren\">(</span><em>col</em>, <em>numBits</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#shiftRightUnsigned\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.shiftRightUnsigned\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Unsigned shift the given value numBits right.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"o\">-</span><span class=\"mi\">42</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">shiftRightUnsigned</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=9223372036854775787)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.shuffle\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">shuffle</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#shuffle\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.shuffle\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: Generates a random permutation of the given array.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The function is non-deterministic.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">],),</span> <span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">shuffle</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span>  <span class=\"c1\"># doctest: +SKIP</span> <span class=\"go\">[Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.signum\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">signum</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.signum\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the signum of the given value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sin\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sin</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.sin\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â angle in radians</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\"><p>sine of the angle, as if computed by <cite>java.lang.Math.sin()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sinh\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sinh</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.sinh\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â hyperbolic angle</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\"><p>hyperbolic sine of the given value, as if computed by <cite>java.lang.Math.sinh()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.size\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">size</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#size\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.size\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns the length of the array or map stored in the column.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],),([</span><span class=\"mi\">1</span><span class=\"p\">],),([],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.slice\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">slice</code><span class=\"sig-paren\">(</span><em>x</em>, <em>start</em>, <em>length</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#slice\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.slice\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: returns an array containing  all the elements in <cite>x</cite> from index <cite>start</cite> (or starting from the end if <cite>start</cite> is negative) with the specified <cite>length</cite>. &gt;&gt;&gt; df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], [âxâ]) &gt;&gt;&gt; df.select(slice(df.x, 2, 2).alias(âslicedâ)).collect() [Row(sliced=[2, 3]), Row(sliced=[5])]</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sort_array\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sort_array</code><span class=\"sig-paren\">(</span><em>col</em>, <em>asc=True</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#sort_array\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.sort_array\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Collection function: sorts the input array in ascending or descending order according to the natural ordering of the array elements. Null elements will be placed at the beginning of the returned array in ascending order or at the end of the returned array in descending order.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â name of column or expression</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([([</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],),([</span><span class=\"mi\">1</span><span class=\"p\">],),([],)],</span> <span class=\"p\">[</span><span class=\"s1\">'data'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">sort_array</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">sort_array</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">asc</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.soundex\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">soundex</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#soundex\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.soundex\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the SoundEx encoding for a string</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s2\">\"Peters\"</span><span class=\"p\">,),(</span><span class=\"s2\">\"Uhrbach\"</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'name'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">soundex</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"soundex\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(soundex='P362'), Row(soundex='U612')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.spark_partition_id\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">spark_partition_id</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#spark_partition_id\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.spark_partition_id\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>A column for partition ID.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This is indeterministic because it depends on data partitioning and task scheduling.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">repartition</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">spark_partition_id</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"pid\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(pid=0), Row(pid=0)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.6.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.split\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">split</code><span class=\"sig-paren\">(</span><em>str</em>, <em>pattern</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#split\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.split\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Splits str around pattern (pattern is a regular expression).</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>pattern is a string represent the regular expression.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'ab12cd'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"s1\">'[0-9]+'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s=['ab', 'cd'])]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.sqrt\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">sqrt</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.sqrt\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Computes the square root of the specified float value.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.struct\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">struct</code><span class=\"sig-paren\">(</span><em>*cols</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#struct\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.struct\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a new struct column.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>cols</strong> â list of column names (string) or list of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> expressions</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">struct</span><span class=\"p\">(</span><span class=\"s1\">'age'</span><span class=\"p\">,</span> <span class=\"s1\">'name'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"struct\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">struct</span><span class=\"p\">([</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"struct\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.substring\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">substring</code><span class=\"sig-paren\">(</span><em>str</em>, <em>pos</em>, <em>len</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#substring\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.substring\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Substring starts at <cite>pos</cite> and is of length <cite>len</cite> when str is String type or returns the slice of byte array that starts at <cite>pos</cite> in byte and is of length <cite>len</cite> when str is Binary type.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The position is not zero based, but 1 based index.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'abcd'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">,])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">substring</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='ab')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.substring_index\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">substring_index</code><span class=\"sig-paren\">(</span><em>str</em>, <em>delim</em>, <em>count</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#substring_index\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.substring_index\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns the substring from string str before count occurrences of the delimiter delim. If count is positive, everything the left of the final delimiter (counting from left) is returned. If count is negative, every to the right of the final delimiter (counting from the right) is returned. substring_index performs a case-sensitive match when searching for delim.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'a.b.c.d'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'s'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">substring_index</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='a.b')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">substring_index</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"s1\">'.'</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'s'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(s='b.c.d')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.tan\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">tan</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.tan\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â angle in radians</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\"><p>tangent of the given value, as if computed by <cite>java.lang.Math.tan()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.tanh\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">tanh</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.tanh\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>col</strong> â hyperbolic angle</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\"><p>hyperbolic tangent of the given value, as if computed by <cite>java.lang.Math.tanh()</cite></p> </dd> </dl> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.toDegrees\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">toDegrees</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.toDegrees\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Deprecated in 2.1, use <a class=\"reference internal\" href=\"#pyspark.sql.functions.degrees\" title=\"pyspark.sql.functions.degrees\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">degrees()</span></code></a> instead.</p> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.toRadians\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">toRadians</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.toRadians\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Deprecated in 2.1, use <a class=\"reference internal\" href=\"#pyspark.sql.functions.radians\" title=\"pyspark.sql.functions.radians\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">radians()</span></code></a> instead.</p> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.to_date\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">to_date</code><span class=\"sig-paren\">(</span><em>col</em>, <em>format=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#to_date\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.to_date\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> of <a class=\"reference internal\" href=\"#pyspark.sql.types.StringType\" title=\"pyspark.sql.types.StringType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.StringType</span></code></a> or <a class=\"reference internal\" href=\"#pyspark.sql.types.TimestampType\" title=\"pyspark.sql.types.TimestampType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.TimestampType</span></code></a> into <a class=\"reference internal\" href=\"#pyspark.sql.types.DateType\" title=\"pyspark.sql.types.DateType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.DateType</span></code></a> using the optionally specified format. Specify formats according to <a class=\"reference external\" href=\"http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\">SimpleDateFormats</a>. By default, it follows casting rules to <a class=\"reference internal\" href=\"#pyspark.sql.types.DateType\" title=\"pyspark.sql.types.DateType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.DateType</span></code></a> if the format is omitted (equivalent to <code class=\"docutils literal notranslate\"><span class=\"pre\">col.cast(\"date\")</span></code>).</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28 10:30:00'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'t'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_date</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'date'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(date=datetime.date(1997, 2, 28))]</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28 10:30:00'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'t'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_date</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"s1\">'yyyy-MM-dd HH:mm:ss'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'date'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(date=datetime.date(1997, 2, 28))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.2.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.to_json\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">to_json</code><span class=\"sig-paren\">(</span><em>col</em>, <em>options={}</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#to_json\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.to_json\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts a column containing a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StructType</span></code>, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ArrayType</span></code> or a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MapType</span></code> into a JSON string. Throws an exception, in the case of an unsupported type.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>col</strong> â name of column containing a struct, an array or a map.</p></li> <li><p><strong>options</strong> â options to control converting. accepts the same options as the JSON datasource</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql</span> <span class=\"k\">import</span> <span class=\"n\">Row</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.types</span> <span class=\"k\">import</span> <span class=\"o\">*</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">Row</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'Alice'</span><span class=\"p\">,</span> <span class=\"n\">age</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json='{\"age\":2,\"name\":\"Alice\"}')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">Row</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'Alice'</span><span class=\"p\">,</span> <span class=\"n\">age</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">Row</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'Bob'</span><span class=\"p\">,</span> <span class=\"n\">age</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)])]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"s2\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Alice\"</span><span class=\"p\">})]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json='{\"name\":\"Alice\"}')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">[{</span><span class=\"s2\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Alice\"</span><span class=\"p\">},</span> <span class=\"p\">{</span><span class=\"s2\">\"name\"</span><span class=\"p\">:</span> <span class=\"s2\">\"Bob\"</span><span class=\"p\">}])]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s2\">\"Alice\"</span><span class=\"p\">,</span> <span class=\"s2\">\"Bob\"</span><span class=\"p\">])]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"s2\">\"key\"</span><span class=\"p\">,</span> <span class=\"s2\">\"value\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_json</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"json\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(json='[\"Alice\",\"Bob\"]')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.1.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.to_timestamp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">to_timestamp</code><span class=\"sig-paren\">(</span><em>col</em>, <em>format=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#to_timestamp\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.to_timestamp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> of <a class=\"reference internal\" href=\"#pyspark.sql.types.StringType\" title=\"pyspark.sql.types.StringType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.StringType</span></code></a> or <a class=\"reference internal\" href=\"#pyspark.sql.types.TimestampType\" title=\"pyspark.sql.types.TimestampType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.TimestampType</span></code></a> into <a class=\"reference internal\" href=\"#pyspark.sql.types.DateType\" title=\"pyspark.sql.types.DateType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.DateType</span></code></a> using the optionally specified format. Specify formats according to <a class=\"reference external\" href=\"http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\">SimpleDateFormats</a>. By default, it follows casting rules to <a class=\"reference internal\" href=\"#pyspark.sql.types.TimestampType\" title=\"pyspark.sql.types.TimestampType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.TimestampType</span></code></a> if the format is omitted (equivalent to <code class=\"docutils literal notranslate\"><span class=\"pre\">col.cast(\"timestamp\")</span></code>).</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28 10:30:00'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'t'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_timestamp</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28 10:30:00'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'t'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_timestamp</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"s1\">'yyyy-MM-dd HH:mm:ss'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.2.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.to_utc_timestamp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">to_utc_timestamp</code><span class=\"sig-paren\">(</span><em>timestamp</em>, <em>tz</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#to_utc_timestamp\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.to_utc_timestamp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given timezone, and renders that timestamp as a timestamp in UTC.</p> <p>However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not timezone-agnostic. So in Spark this function just shift the timestamp value from the given timezone to UTC timezone.</p> <p>This function may return confusing result if the input is a string with timezone, e.g. â2018-03-13T06:18:23+00:00â. The reason is that, Spark firstly cast the string to timestamp according to the timezone in the string, and finally display the result by converting the timestamp to string according to the session local timezone.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>timestamp</strong> â the column that contains timestamps</p></li> <li><p><strong>tz</strong> â a string that has the ID of timezone, e.g. âGMTâ, âAmerica/Los_Angelesâ, etc</p></li> </ul> </dd> </dl> <div class=\"versionchanged\"> <p><span class=\"versionmodified changed\">Changed in version 2.4: </span><cite>tz</cite> can take a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> containing timezone ID strings.</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28 10:30:00'</span><span class=\"p\">,</span> <span class=\"s1\">'JST'</span><span class=\"p\">)],</span> <span class=\"p\">[</span><span class=\"s1\">'ts'</span><span class=\"p\">,</span> <span class=\"s1\">'tz'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_utc_timestamp</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">ts</span><span class=\"p\">,</span> <span class=\"s2\">\"PST\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'utc_time'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">to_utc_timestamp</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">ts</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">tz</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'utc_time'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.translate\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">translate</code><span class=\"sig-paren\">(</span><em>srcCol</em>, <em>matching</em>, <em>replace</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#translate\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.translate\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>A function translate any character in the <cite>srcCol</cite> by a character in <cite>matching</cite>. The characters in <cite>replace</cite> is corresponding to the characters in <cite>matching</cite>. The translate will happen when any character in the string matching with the character in the <cite>matching</cite>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'translate'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">translate</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">,</span> <span class=\"s2\">\"rnlt\"</span><span class=\"p\">,</span> <span class=\"s2\">\"123\"</span><span class=\"p\">)</span> \\ <span class=\"gp\">... </span>    <span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'r'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(r='1a2s3ae')]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.trim\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">trim</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.trim\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Trim the spaces from both ends for the specified string column.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.trunc\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">trunc</code><span class=\"sig-paren\">(</span><em>date</em>, <em>format</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#trunc\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.trunc\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Returns date truncated to the unit specified by the format.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><p><strong>format</strong> â âyearâ, âyyyyâ, âyyâ or âmonthâ, âmonâ, âmmâ</p> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'1997-02-28'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'d'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">trunc</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"s1\">'year'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'year'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(year=datetime.date(1997, 1, 1))]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">trunc</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"s1\">'mon'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'month'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(month=datetime.date(1997, 2, 1))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.udf\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">udf</code><span class=\"sig-paren\">(</span><em>f=None</em>, <em>returnType=StringType</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#udf\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.udf\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Creates a user defined function (UDF).</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The user-defined functions are considered deterministic by default. Due to optimization, duplicate invocations may be eliminated or the function may even be invoked more times than it is present in the query. If your function is not deterministic, call <cite>asNondeterministic</cite> on the user defined function. E.g.:</p> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.types</span> <span class=\"k\">import</span> <span class=\"n\">IntegerType</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">import</span> <span class=\"nn\">random</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">random_udf</span> <span class=\"o\">=</span> <span class=\"n\">udf</span><span class=\"p\">(</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"mi\">100</span><span class=\"p\">),</span> <span class=\"n\">IntegerType</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">asNondeterministic</span><span class=\"p\">()</span> </pre></div> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The user-defined functions do not support conditional expressions or short circuiting in boolean expressions and it ends up with being executed all internally. If the functions can fail on special rows, the workaround is to incorporate the condition into the functions.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The user-defined functions do not take keyword arguments on the calling side.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>f</strong> â python function if used as a standalone function</p></li> <li><p><strong>returnType</strong> â the return type of the user-defined function. The value can be either a <a class=\"reference internal\" href=\"#pyspark.sql.types.DataType\" title=\"pyspark.sql.types.DataType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.DataType</span></code></a> object or a DDL-formatted type string.</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"kn\">from</span> <span class=\"nn\">pyspark.sql.types</span> <span class=\"k\">import</span> <span class=\"n\">IntegerType</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">slen</span> <span class=\"o\">=</span> <span class=\"n\">udf</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">s</span><span class=\"p\">:</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">),</span> <span class=\"n\">IntegerType</span><span class=\"p\">())</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">udf</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">to_upper</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"k\">if</span> <span class=\"n\">s</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span> <span class=\"gp\">... </span>        <span class=\"k\">return</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">upper</span><span class=\"p\">()</span> <span class=\"gp\">...</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"p\">:</span><span class=\"n\">udf</span><span class=\"p\">(</span><span class=\"n\">returnType</span><span class=\"o\">=</span><span class=\"n\">IntegerType</span><span class=\"p\">())</span> <span class=\"gp\">... </span><span class=\"k\">def</span> <span class=\"nf\">add_one</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span> <span class=\"gp\">... </span>    <span class=\"k\">if</span> <span class=\"n\">x</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span> <span class=\"gp\">... </span>        <span class=\"k\">return</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"gp\">...</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s2\">\"John Doe\"</span><span class=\"p\">,</span> <span class=\"mi\">21</span><span class=\"p\">)],</span> <span class=\"p\">(</span><span class=\"s2\">\"id\"</span><span class=\"p\">,</span> <span class=\"s2\">\"name\"</span><span class=\"p\">,</span> <span class=\"s2\">\"age\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">slen</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"slen(name)\"</span><span class=\"p\">),</span> <span class=\"n\">to_upper</span><span class=\"p\">(</span><span class=\"s2\">\"name\"</span><span class=\"p\">),</span> <span class=\"n\">add_one</span><span class=\"p\">(</span><span class=\"s2\">\"age\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> <span class=\"go\">+----------+--------------+------------+</span> <span class=\"go\">|slen(name)|to_upper(name)|add_one(age)|</span> <span class=\"go\">+----------+--------------+------------+</span> <span class=\"go\">|         8|      JOHN DOE|          22|</span> <span class=\"go\">+----------+--------------+------------+</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.3.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.unbase64\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">unbase64</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.unbase64\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Decodes a BASE64 encoded string column and returns it as a binary column.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.unhex\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">unhex</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#unhex\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.unhex\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of number.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'414243'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'a'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">unhex</span><span class=\"p\">(</span><span class=\"s1\">'a'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(unhex(a)=bytearray(b'ABC'))]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.unix_timestamp\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">unix_timestamp</code><span class=\"sig-paren\">(</span><em>timestamp=None</em>, <em>format='yyyy-MM-dd HH:mm:ss'</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#unix_timestamp\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.unix_timestamp\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Convert time string with given pattern (âyyyy-MM-dd HH:mm:ssâ, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail.</p> <p>if <cite>timestamp</cite> is None, then it returns current timestamp.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">conf</span><span class=\"o\">.</span><span class=\"n\">set</span><span class=\"p\">(</span><span class=\"s2\">\"spark.sql.session.timeZone\"</span><span class=\"p\">,</span> <span class=\"s2\">\"America/Los_Angeles\"</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">time_df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">time_df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">unix_timestamp</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">,</span> <span class=\"s1\">'yyyy-MM-dd'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'unix_time'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(unix_time=1428476400)]</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">conf</span><span class=\"o\">.</span><span class=\"n\">unset</span><span class=\"p\">(</span><span class=\"s2\">\"spark.sql.session.timeZone\"</span><span class=\"p\">)</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.upper\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">upper</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#pyspark.sql.functions.upper\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Converts a string column to upper case.</p> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.weekofyear\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">weekofyear</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#weekofyear\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.weekofyear\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the week number of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">weekofyear</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">dt</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'week'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(week=15)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.when\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">when</code><span class=\"sig-paren\">(</span><em>condition</em>, <em>value</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#when\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.when\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions. If <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">Column.otherwise()</span></code> is not invoked, None is returned for unmatched conditions.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\"><ul class=\"simple\"> <li><p><strong>condition</strong> â a boolean <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> expression.</p></li> <li><p><strong>value</strong> â a literal value, or a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Column</span></code> expression.</p></li> </ul> </dd> </dl> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">when</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'age'</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">otherwise</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"age\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(age=3), Row(age=4)]</span> </pre></div> </div> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">when</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span> <span class=\"o\">==</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">age</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"age\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(age=3), Row(age=None)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.4.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.window\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">window</code><span class=\"sig-paren\">(</span><em>timeColumn</em>, <em>windowDuration</em>, <em>slideDuration=None</em>, <em>startTime=None</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#window\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.window\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported.</p> <p>The time column must be of <a class=\"reference internal\" href=\"#pyspark.sql.types.TimestampType\" title=\"pyspark.sql.types.TimestampType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.TimestampType</span></code></a>.</p> <p>Durations are provided as strings, e.g. â1 secondâ, â1 day 12 hoursâ, â2 minutesâ. Valid interval strings are âweekâ, âdayâ, âhourâ, âminuteâ, âsecondâ, âmillisecondâ, âmicrosecondâ. If the <code class=\"docutils literal notranslate\"><span class=\"pre\">slideDuration</span></code> is not provided, the windows will be tumbling windows.</p> <p>The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start window intervals. For example, in order to have hourly tumbling windows that start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15â¦ provide <cite>startTime</cite> as <cite>15 minutes</cite>.</p> <p>The output column will be a struct called âwindowâ by default with the nested columns âstartâ and âendâ, where âstartâ and âendâ will be of <a class=\"reference internal\" href=\"#pyspark.sql.types.TimestampType\" title=\"pyspark.sql.types.TimestampType\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pyspark.sql.types.TimestampType</span></code></a>.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s2\">\"2016-03-11 09:00:07\"</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)])</span><span class=\"o\">.</span><span class=\"n\">toDF</span><span class=\"p\">(</span><span class=\"s2\">\"date\"</span><span class=\"p\">,</span> <span class=\"s2\">\"val\"</span><span class=\"p\">)</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">groupBy</span><span class=\"p\">(</span><span class=\"n\">window</span><span class=\"p\">(</span><span class=\"s2\">\"date\"</span><span class=\"p\">,</span> <span class=\"s2\">\"5 seconds\"</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"s2\">\"val\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"sum\"</span><span class=\"p\">))</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">window</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"s2\">\"string\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"start\"</span><span class=\"p\">),</span> <span class=\"gp\">... </span>         <span class=\"n\">w</span><span class=\"o\">.</span><span class=\"n\">window</span><span class=\"o\">.</span><span class=\"n\">end</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"s2\">\"string\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s2\">\"end\"</span><span class=\"p\">),</span> <span class=\"s2\">\"sum\"</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 2.0.</span></p> </div> </dd></dl> \n",
    "\n",
    "<dl class=\"function\"> <dt id=\"pyspark.sql.functions.year\"> <code class=\"descclassname\">pyspark.sql.functions.</code><code class=\"descname\">year</code><span class=\"sig-paren\">(</span><em>col</em><span class=\"sig-paren\">)</span><a class=\"reference internal\" href=\"_modules/pyspark/sql/functions.html#year\"><span class=\"viewcode-link\">[source]</span></a><a class=\"headerlink\" href=\"#pyspark.sql.functions.year\" title=\"Permalink to this definition\">Â¶</a></dt> <dd><p>Extract the year of a given date as integer.</p> <div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"p\">([(</span><span class=\"s1\">'2015-04-08'</span><span class=\"p\">,)],</span> <span class=\"p\">[</span><span class=\"s1\">'dt'</span><span class=\"p\">])</span> <span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">year</span><span class=\"p\">(</span><span class=\"s1\">'dt'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">alias</span><span class=\"p\">(</span><span class=\"s1\">'year'</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"p\">()</span> <span class=\"go\">[Row(year=2015)]</span> </pre></div> </div> <div class=\"versionadded\"> <p><span class=\"versionmodified added\">New in version 1.5.</span></p> </div> </dd></dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
