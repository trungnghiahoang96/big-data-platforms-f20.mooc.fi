{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nghiaht7/.sdkman/candidates/spark/current'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "key = \"SPARK_HOME\"\n",
    "\n",
    "spark_home = os.environ.get(key)\n",
    "PATH = spark_home + \"/data/mllib/images/origin/kittens\"\n",
    "os.environ.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: [SPARK_LOCAL_CONFIGS]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nghiaht7/.sdkman/candidates/spark/3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/11 20:28:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/11 20:28:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/10/11 20:28:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "21/10/11 20:28:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "21/10/11 20:28:35 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "21/10/11 20:28:37 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "sample_libsvm_data = spark.read.format(\"libsvm\").load(\n",
    "    \"/home/nghiaht7/.sdkman/candidates/spark/current/data/mllib/sample_libsvm_data.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "sample_libsvm_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.classification.DecisionTreeClassifier'>\n",
      "<class 'pyspark.ml.feature.VectorIndexerModel'>\n",
      "<class 'pyspark.ml.feature.StringIndexerModel'>\n"
     ]
    }
   ],
   "source": [
    "# Creating our stages:\n",
    "\n",
    "# STAGE 1:\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(\n",
    "    sample_libsvm_data\n",
    ")\n",
    "\n",
    "# STAGE 2:\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "feature_indexer = VectorIndexer(\n",
    "    inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4\n",
    ").fit(sample_libsvm_data)\n",
    "\n",
    "# STAGE 3:\n",
    "# Train a DecisionTree model.\n",
    "decission_tree_classifier_model = DecisionTreeClassifier(\n",
    "    labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\"\n",
    ")\n",
    "\n",
    "print(type(decission_tree_classifier_model))\n",
    "print(type(feature_indexer))\n",
    "print(type(label_indexer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Pipeline:\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        label_indexer,  # STAGE 1\n",
    "        feature_indexer,  # STAGE 2\n",
    "        decission_tree_classifier_model,  # STAGE 3\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[95,96,97,12...|\n",
      "|       1.0|         1.0|(692,[122,123,148...|\n",
      "|       1.0|         1.0|(692,[125,126,127...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.03571 \n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(training_data, test_data) = sample_libsvm_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Error = {1.0 - accuracy:.5f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexerModel: uid=StringIndexer_25ccc4404185, handleInvalid=error, VectorIndexerModel: uid=VectorIndexer_9a8ae6fac5d6, numFeatures=692, handleInvalid=error, DecisionTreeClassifier_85d4bb399116]\n",
      "[StringIndexerModel: uid=StringIndexer_25ccc4404185, handleInvalid=error, VectorIndexerModel: uid=VectorIndexer_9a8ae6fac5d6, numFeatures=692, handleInvalid=error, DecisionTreeClassificationModel: uid=DecisionTreeClassifier_85d4bb399116, depth=1, numNodes=3, numClasses=2, numFeatures=692]\n"
     ]
    }
   ],
   "source": [
    "# You can see that the Pipeline and the PipelineModel have the same stages\n",
    "print(pipeline.getStages())\n",
    "print(model.stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "sample_libsvm_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our stages:\n",
    "\n",
    "# STAGE 1:\n",
    "# Automatically identify categorical features, and index them.\n",
    "feature_indexer = VectorIndexer(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"indexedFeatures\",\n",
    "    # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "    maxCategories=4,\n",
    ").fit(sample_libsvm_data)\n",
    "\n",
    "# STAGE 2:\n",
    "# Train a RandomForest model.\n",
    "random_forest_model = RandomForestRegressor(featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Pipeline:\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[feature_indexer, random_forest_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[121,122,123...|\n",
      "|       0.0|  0.0|(692,[122,123,124...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|      0.05|  0.0|(692,[124,125,126...|\n",
      "|       0.1|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.075119\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(training_data, test_data) = sample_libsvm_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VectorIndexer_807bb456cf66, RandomForestRegressor_97a60d492d9e]\n",
      "[VectorIndexer_807bb456cf66, RandomForestRegressionModel (uid=RandomForestRegressor_97a60d492d9e) with 20 trees]\n"
     ]
    }
   ],
   "source": [
    "# You can see that the Pipeline and the PipelineModel have the same stages\n",
    "print(pipeline.getStages())\n",
    "print(model.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressionModel (uid=RandomForestRegressor_97a60d492d9e) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "# The last stage in a PipelineModel is usually the most informative\n",
    "print(model.stages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline: <class 'pyspark.ml.pipeline.Pipeline'>\n",
      "model: <class 'pyspark.ml.pipeline.PipelineModel'>\n"
     ]
    }
   ],
   "source": [
    "# Here you can see that pipeline and model are Pipeline and PipelineModel classes\n",
    "print(\"pipeline:\", type(pipeline))\n",
    "print(\"model:\", type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
