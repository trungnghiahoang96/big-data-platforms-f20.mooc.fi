{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features\n",
    "---\n",
    "MLlib Main Guide: https://spark.apache.org/docs/2.4.3/ml-features.html\n",
    "\n",
    "This module contains algorithms for working with features, roughly divided into these groups:\n",
    "\n",
    "- Extraction: Extracting features from “raw” data\n",
    "- Transformation: Scaling, converting, or modifying features\n",
    "- Selection: Selecting a subset from a larger set of features\n",
    "- Locality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms.\n",
    "\n",
    "## pyspark.ml.feature\n",
    "Class structure: https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#module-pyspark.ml.feature  \n",
    "GitHub: https://github.com/apache/spark/blob/v2.4.3/python/pyspark/ml/feature.py\n",
    "\n",
    "### [Feature Extractors](https://spark.apache.org/docs/2.4.3/ml-features.html#feature-extractors)\n",
    "\n",
    "- [HashingTF](https://spark.apache.org/docs/2.4.3/ml-features.html#tf-idf)\n",
    "- [IDF & IDFModel](https://spark.apache.org/docs/2.4.3/ml-features.html#tf-idf)\n",
    "- [Word2Vec & Word2VecModel](https://spark.apache.org/docs/2.4.3/ml-features.html#word2vec)\n",
    "- [CountVectorizer & CountVectorizerModel](https://spark.apache.org/docs/2.4.3/ml-features.html#countvectorizer)\n",
    "- [FeatureHasher](https://spark.apache.org/docs/2.4.3/ml-features.html#featurehasher)\n",
    "\n",
    "\n",
    "### [Feature Transformers](https://spark.apache.org/docs/2.4.3/ml-features.html#feature-transformers)\n",
    "\n",
    "- [Tokenizer](https://spark.apache.org/docs/2.4.3/ml-features.html#tokenizer)\n",
    "- [RegexTokenizer](https://spark.apache.org/docs/2.4.3/ml-features.html#tokenizer)\n",
    "- [StopWordsRemover](https://spark.apache.org/docs/2.4.3/ml-features.html#stopwordsremover)\n",
    "- [NGram](https://spark.apache.org/docs/2.4.3/ml-features.html#n-gram)\n",
    "- [Binarizer](https://spark.apache.org/docs/2.4.3/ml-features.html#binarizer)\n",
    "- [PCA](https://spark.apache.org/docs/2.4.3/ml-features.html#pca)\n",
    "- [PCAModel](https://spark.apache.org/docs/2.4.3/ml-features.html#pca)\n",
    "- [PolynomialExpansion](https://spark.apache.org/docs/2.4.3/ml-features.html#polynomialexpansion)\n",
    "- [DCT](https://spark.apache.org/docs/2.4.3/ml-features.html#discrete-cosine-transform-dct)\n",
    "- [StringIndexer & StringIndexerModel](https://spark.apache.org/docs/2.4.3/ml-features.html#stringindexer)\n",
    "- [IndexToString](https://spark.apache.org/docs/2.4.3/ml-features.html#indextostring)\n",
    "- [OneHotEncoder & OneHotEncoderModel](https://spark.apache.org/docs/2.4.3/ml-features.html#onehotencoder-deprecated-since-230)\n",
    "- [OneHotEncoderEstimator](https://spark.apache.org/docs/2.4.3/ml-features.html#onehotencoderestimator)\n",
    "- [VectorIndexer](https://spark.apache.org/docs/2.4.3/ml-features.html#vectorindexer)\n",
    "- [VectorIndexerModel](https://spark.apache.org/docs/2.4.3/ml-features.html#vectorindexer)\n",
    "- [Normalizer](https://spark.apache.org/docs/2.4.3/ml-features.html#normalizer)\n",
    "- [StandardScaler & StandardScalerModel](https://spark.apache.org/docs/2.4.3/ml-features.html#standardscaler)\n",
    "- [MinMaxScaler & MinMaxScalerModel](https://spark.apache.org/docs/2.4.3/ml-features.html#minmaxscaler)\n",
    "- [MaxAbsScaler & MaxAbsScalerModel](https://spark.apache.org/docs/2.4.3/ml-features.html#maxabsscaler)\n",
    "- [Bucketizer](https://spark.apache.org/docs/2.4.3/ml-features.html#bucketizer)\n",
    "- [ElementwiseProduct](https://spark.apache.org/docs/2.4.3/ml-features.html#elementwiseproduct)\n",
    "- [SQLTransformer](https://spark.apache.org/docs/2.4.3/ml-features.html#sqltransformer)\n",
    "- [VectorAssembler](https://spark.apache.org/docs/2.4.3/ml-features.html#vectorassembler)\n",
    "- [VectorSizeHint](https://spark.apache.org/docs/2.4.3/ml-features.html#vectorsizehint)\n",
    "- [QuantileDiscretizer](https://spark.apache.org/docs/2.4.3/ml-features.html#quantilediscretizer)\n",
    "- [Imputer & ImputerModel](https://spark.apache.org/docs/2.4.3/ml-features.html#imputer)\n",
    "\n",
    "Not available in Python:\n",
    "- [Interaction](https://spark.apache.org/docs/2.4.3/ml-features.html#interaction)\n",
    "\n",
    "## [Feature Selectors](https://spark.apache.org/docs/2.4.3/ml-features.html#feature-selectors)\n",
    "\n",
    "- [VectorSlicer](https://spark.apache.org/docs/2.4.3/ml-features.html#vectorslicer)\n",
    "- [RFormula & RFormulaModel](https://spark.apache.org/docs/2.4.3/ml-features.html#rformula)\n",
    "- [ChiSqSelector & ChiSqSelectorModel](https://spark.apache.org/docs/2.4.3/ml-features.html#chisqselector)\n",
    "\n",
    "\n",
    "## [Locality Sensitive Hashing (LSH)](https://spark.apache.org/docs/2.4.3/ml-features.html#locality-sensitive-hashing)\n",
    "\n",
    "- [BucketedRandomProjectionLSH & BucketedRandomProjectionLSHModel](https://spark.apache.org/docs/2.4.3/ml-features.html#bucketed-random-projection-for-euclidean-distance)\n",
    "- [MinHashLSH & MinHashLSHModel](https://spark.apache.org/docs/2.4.3/ml-features.html#minhash-for-jaccard-distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|hour|result|\n",
      "+---+----+------+\n",
      "|  0|18.0|   2.0|\n",
      "|  1|19.0|   2.0|\n",
      "|  2| 8.0|   1.0|\n",
      "|  3| 5.0|   1.0|\n",
      "|  4| 2.2|   0.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "data = [(0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.2)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"hour\"])\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"hour\", outputCol=\"result\")\n",
    "\n",
    "result = discretizer.fit(df).transform(df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['Hi', 'I', 'heard', 'about', 'Spark'] => \n",
      "Vector: [0.01781592555344105,0.02203895077109337,0.02248857170343399]\n",
      "\n",
      "Text: ['I', 'wish', 'Java', 'could', 'use', 'case', 'classes'] => \n",
      "Vector: [-0.00022016598709991998,-0.0291545108027224,-0.01409794549856867]\n",
      "\n",
      "Text: ['Logistic', 'regression', 'models', 'are', 'neat'] => \n",
      "Vector: [-0.03180776406079531,0.0591656094416976,0.005205358564853668]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Hi I heard about Spark\".split(\" \"),),\n",
    "        (\"I wish Java could use case classes\".split(\" \"),),\n",
    "        (\"Logistic regression models are neat\".split(\" \"),),\n",
    "    ],\n",
    "    [\"text\"],\n",
    ")\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(f\"Text: {text} => \\nVector: {vector}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "|real|bool |stringNum|string|features                                                |\n",
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "|2.2 |true |1        |foo   |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n",
      "|3.3 |false|2        |bar   |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n",
      "|4.4 |false|3        |baz   |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n",
      "|5.5 |false|4        |foo   |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0]) |\n",
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame(\n",
    "    [\n",
    "        (2.2, True, \"1\", \"foo\"),\n",
    "        (3.3, False, \"2\", \"bar\"),\n",
    "        (4.4, False, \"3\", \"baz\"),\n",
    "        (5.5, False, \"4\", \"foo\"),\n",
    "    ],\n",
    "    [\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    ")\n",
    "\n",
    "hasher = FeatureHasher(\n",
    "    inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"], outputCol=\"features\"\n",
    ")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          3.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "|  6|       d|          2.0|\n",
      "|  7|       a|          0.0|\n",
      "|  8|       d|          2.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (0, \"a\"),\n",
    "        (1, \"b\"),\n",
    "        (2, \"c\"),\n",
    "        (3, \"a\"),\n",
    "        (4, \"a\"),\n",
    "        (5, \"c\"),\n",
    "        (6, \"d\"),\n",
    "        (7, \"a\"),\n",
    "        (8, \"d\"),\n",
    "    ],\n",
    "    [\"id\", \"category\"],\n",
    ")\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose 351 categorical features: ['645', '69', '365', '138', '101', '479', '333', '249', '0', '555', '666', '88', '170', '115', '276', '308', '5', '449', '120', '247', '614', '677', '202', '10', '56', '533', '142', '500', '340', '670', '174', '42', '417', '24', '37', '25', '257', '389', '52', '14', '504', '110', '587', '619', '196', '559', '638', '20', '421', '46', '93', '284', '228', '448', '57', '78', '29', '475', '164', '591', '646', '253', '106', '121', '84', '480', '147', '280', '61', '221', '396', '89', '133', '116', '1', '507', '312', '74', '307', '452', '6', '248', '60', '117', '678', '529', '85', '201', '220', '366', '534', '102', '334', '28', '38', '561', '392', '70', '424', '192', '21', '137', '165', '33', '92', '229', '252', '197', '361', '65', '97', '665', '583', '285', '224', '650', '615', '9', '53', '169', '593', '141', '610', '420', '109', '256', '225', '339', '77', '193', '669', '476', '642', '637', '590', '679', '96', '393', '647', '173', '13', '41', '503', '134', '73', '105', '2', '508', '311', '558', '674', '530', '586', '618', '166', '32', '34', '148', '45', '161', '279', '64', '689', '17', '149', '584', '562', '176', '423', '191', '22', '44', '59', '118', '281', '27', '641', '71', '391', '12', '445', '54', '313', '611', '144', '49', '335', '86', '672', '172', '113', '681', '219', '419', '81', '230', '362', '451', '76', '7', '39', '649', '98', '616', '477', '367', '535', '103', '140', '621', '91', '66', '251', '668', '198', '108', '278', '223', '394', '306', '135', '563', '226', '3', '505', '80', '167', '35', '473', '675', '589', '162', '531', '680', '255', '648', '112', '617', '194', '145', '48', '557', '690', '63', '640', '18', '282', '95', '310', '50', '67', '199', '673', '16', '585', '502', '338', '643', '31', '336', '613', '11', '72', '175', '446', '612', '143', '43', '250', '231', '450', '99', '363', '556', '87', '203', '671', '688', '104', '368', '588', '40', '304', '26', '258', '390', '55', '114', '171', '139', '418', '23', '8', '75', '119', '58', '667', '478', '536', '82', '620', '447', '36', '168', '146', '30', '51', '190', '19', '422', '564', '305', '107', '4', '136', '506', '79', '195', '474', '664', '532', '94', '283', '395', '332', '528', '644', '47', '15', '163', '200', '68', '62', '277', '691', '501', '90', '111', '254', '227', '337', '122', '83', '309', '560', '639', '676', '222', '592', '364', '100']\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|             indexed|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "data = spark.read.format(\"libsvm\").load(\n",
    "    \"/usr/local/spark-2.4.3-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\"\n",
    ")\n",
    "\n",
    "indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=10)\n",
    "indexerModel = indexer.fit(data)\n",
    "\n",
    "categoricalFeatures = indexerModel.categoryMaps\n",
    "print(\n",
    "    f\"Chose {len(categoricalFeatures)} categorical \"\n",
    "    f\"features: {[str(k) for k in categoricalFeatures.keys()]}\"\n",
    ")\n",
    "\n",
    "# Create new column \"indexed\" with categorical values transformed to indices\n",
    "indexedData = indexerModel.transform(data)\n",
    "indexedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|    -0.4|             1.0|\n",
      "|    -0.2|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.1|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|     0.3|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "data = [\n",
    "    (-999.9,),\n",
    "    (-0.5,),\n",
    "    (-0.3,),\n",
    "    (-0.4,),\n",
    "    (-0.2,),\n",
    "    (0.0,),\n",
    "    (0.1,),\n",
    "    (0.2,),\n",
    "    (0.3,),\n",
    "    (999.9,),\n",
    "]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "bucketizer = Bucketizer(\n",
    "    splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\"\n",
    ")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "print(f\"Bucketizer output with {len(bucketizer.getSplits())-1} buckets\")\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
