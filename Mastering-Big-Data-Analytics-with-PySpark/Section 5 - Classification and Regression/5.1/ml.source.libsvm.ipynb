{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBSVM Data Source\n",
    "\n",
    "This notebook contains a slightly modified version of the Java API docs available here: https://spark.apache.org/docs/2.3.4/api/java/org/apache/spark/ml/source/libsvm/LibSVMDataSource.html\n",
    "\n",
    "Below examples were adapted to work with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libsvm package implements Spark SQL data source API for loading LIBSVM data as DataFrame. \n",
    "\n",
    "The loaded DataFrame has two columns: \n",
    "- `label` containing labels stored as doubles\n",
    "- `features` containing feature vectors stored as Vectors\n",
    "\n",
    "To use LIBSVM data source, you need to set \"libsvm\" as the format in DataFrameReader and optionally specify options, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/usr/local/spark-2.4.3-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\"\n",
    "\n",
    "df = spark.read.format(\"libsvm\").option(\"numFeatures\", \"780\").load(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIBSVM data source supports the following options: \n",
    "- `numFeatures`: number of features. If unspecified or nonpositive, the number of features will be determined automatically at the cost of one additional pass. This is also useful when the dataset is already split into multiple files and you want to load them separately, because some features may not present in certain files, which leads to inconsistent feature dimensions. \n",
    "- `vectorType`: feature vector type, \"sparse\" (default) or \"dense\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(780,[127,128,129...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[124,125,126...|\n",
      "|  1.0|(780,[152,153,154...|\n",
      "|  1.0|(780,[151,152,153...|\n",
      "|  0.0|(780,[129,130,131...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[99,100,101,...|\n",
      "|  0.0|(780,[154,155,156...|\n",
      "|  0.0|(780,[127,128,129...|\n",
      "|  1.0|(780,[154,155,156...|\n",
      "|  0.0|(780,[153,154,155...|\n",
      "|  0.0|(780,[151,152,153...|\n",
      "|  1.0|(780,[129,130,131...|\n",
      "|  0.0|(780,[154,155,156...|\n",
      "|  1.0|(780,[150,151,152...|\n",
      "|  0.0|(780,[124,125,126...|\n",
      "|  0.0|(780,[152,153,154...|\n",
      "|  1.0|(780,[97,98,99,12...|\n",
      "|  1.0|(780,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
