{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: [SPARK_LOCAL_CONFIGS]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nghiaht7/.sdkman/candidates/spark/3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/11 19:44:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/11 19:44:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/10/11 19:44:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "21/10/11 19:44:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "21/10/11 19:44:57 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Guide: https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#module-pyspark.ml.linalg\n",
    "\n",
    "MLlib utilities for linear algebra.  \n",
    "\n",
    "Examples below were taken from the RDD-based section of the documentation and slightly modified  \n",
    "https://spark.apache.org/docs/latest/mllib-data-types.html#local-vector  \n",
    "https://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors\n",
    "For dense vectors, MLlib uses the __NumPy array__ type, so you can simply pass NumPy arrays around.  \n",
    "For sparse vectors, you can construct a SparseVector object from MLlib or pass SciPy __scipy.sparse__ column vectors if __SciPy__ is available in the environment.\n",
    "\n",
    "Docs (RDD-section): https://spark.apache.org/docs/latest/mllib-data-types.html#local-vector  \n",
    "\n",
    "> A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector.\n",
    "\n",
    "The following classes exist in `pyspark.ml.linalg` for Vectors:\n",
    "\n",
    "`Vectors` (_main class to use_)  \n",
    "> Factory methods for working with vectors.  \n",
    "> .. note:: Dense vectors are simply represented as NumPy array objects, so there is no need to covert them for use in MLlib. For sparse vectors, the factory methods in this class create an MLlib-compatible type, or users can pass in SciPy's C{scipy.sparse} column vectors.\n",
    "\n",
    "- `DenseVector`  \n",
    "\t> A dense vector represented by a value array. We use numpy array for storage and arithmetics will be delegated to the underlying numpy array.\n",
    "\n",
    "- `SparseVector`  \n",
    "\t> A simple sparse vector class for passing data to MLlib. Users may alternatively pass SciPy's {scipy.sparse} data types.\n",
    "\n",
    "`VectorUDT`  \n",
    "> SQL user-defined type (UDT) for Vector.\n",
    "\n",
    "`Vector`  \n",
    "> Abstract class for DenseVector and SparseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Vector Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_783839/2198361722.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Use a NumPy array as a dense vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Use a NumPy array as a dense vector.\n",
    "dv1 = np.array([1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a SparseVector.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "\n",
    "print(dv1)\n",
    "print(sv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Vector Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 3.0]\n",
      "  (0, 0)\t1.0\n",
      "  (2, 0)\t3.0\n"
     ]
    }
   ],
   "source": [
    "# Use a Python list as a dense vector.\n",
    "dv2 = [1.0, 0.0, 3.0]\n",
    "\n",
    "# Use a single-column SciPy csc_matrix as a sparse vector.\n",
    "sv2 = sps.csc_matrix(\n",
    "    (np.array([1.0, 3.0]), np.array([0, 2]), np.array([0, 2])), shape=(3, 1)\n",
    ")\n",
    "\n",
    "print(dv2)\n",
    "print(sv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices\n",
    "\n",
    "Docs (RDD-section): https://spark.apache.org/docs/latest/mllib-data-types.html#local-matrix\n",
    "\n",
    "> The base class of local matrices is _Matrix_, and we provide two implementations: _DenseMatrix_, and _SparseMatrix_. We recommend using the factory methods implemented in _Matrices_ to create local matrices. Remember, local matrices in MLlib are stored in column-major order.\n",
    "\n",
    "The following classes exist in `pyspark.ml.linalg` for Matrices:\n",
    "\n",
    "Matrices (_main class to use_)  \n",
    "> Factory methods for working with matrices.\n",
    "\n",
    "- DenseMatrix  \n",
    "> Column-major dense matrix.\n",
    "\n",
    "- SparseMatrix  \n",
    "> Sparse Matrix stored in CSC format.\n",
    "\n",
    "MatrixUDT  \n",
    "> SQL user-defined type (UDT) for Matrix.\n",
    "\t\n",
    "Matrix  \n",
    "> Represents a local matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[1., 2.],\n",
      "             [3., 4.],\n",
      "             [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm2 = Matrices.dense(3, 2, [1, 3, 5, 2, 4, 6])\n",
    "print(dm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 X 2 CSCMatrix\n",
      "(0,0) 9.0\n",
      "(2,1) 6.0\n",
      "(1,1) 8.0\n"
     ]
    }
   ],
   "source": [
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])\n",
    "print(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
