{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning\n",
    "\n",
    "A short revisit on Hyper Parameter Tuning (first described in Section 4) with an example applied to a LogisticRegression Pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset used for training (labeled):\n",
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "|  4|     b spark who|  1.0|\n",
      "|  5|         g d a y|  0.0|\n",
      "|  6|       spark fly|  1.0|\n",
      "|  7|   was mapreduce|  0.0|\n",
      "|  8| e spark program|  1.0|\n",
      "|  9|         a e c l|  0.0|\n",
      "| 10|   spark compile|  1.0|\n",
      "| 11| hadoop software|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n",
      "Dataset used for testing (unlabeled):\n",
      "+---+---------------+\n",
      "| id|           text|\n",
      "+---+---------------+\n",
      "|  4|    spark i j k|\n",
      "|  5|          l m n|\n",
      "|  6|mapreduce spark|\n",
      "|  7|  apache hadoop|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training_data = [\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0),\n",
    "]\n",
    "training = spark.createDataFrame(training_data, [\"id\", \"text\", \"label\"])\n",
    "print(\"Dataset used for training (labeled):\")\n",
    "training.show()\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test_data = [\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\"),\n",
    "]\n",
    "test = spark.createDataFrame(test_data, [\"id\", \"text\"],)\n",
    "print(\"Dataset used for testing (unlabeled):\")\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------------------------------------+----------+\n",
      "|id |text           |probability                             |prediction|\n",
      "+---+---------------+----------------------------------------+----------+\n",
      "|4  |spark i j k    |[0.2580684222584647,0.7419315777415353] |1.0       |\n",
      "|5  |l m n          |[0.9185597412653915,0.08144025873460846]|0.0       |\n",
      "|6  |mapreduce spark|[0.43203205663918753,0.5679679433608125]|1.0       |\n",
      "|7  |apache hadoop  |[0.6766082856652201,0.3233917143347798] |0.0       |\n",
      "+---+---------------+----------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000])\n",
    "    .addGrid(lr.regParam, [0.1, 0.01])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=BinaryClassificationEvaluator(),\n",
    "    numFolds=2,\n",
    ")  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "selected.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "|  4|     b spark who|  1.0|\n",
      "|  5|         g d a y|  0.0|\n",
      "|  6|       spark fly|  1.0|\n",
      "|  7|   was mapreduce|  0.0|\n",
      "|  8| e spark program|  1.0|\n",
      "|  9|         a e c l|  0.0|\n",
      "| 10|   spark compile|  1.0|\n",
      "| 11| hadoop software|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
