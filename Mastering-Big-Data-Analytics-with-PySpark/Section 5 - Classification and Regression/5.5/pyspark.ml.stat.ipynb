{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistics\n",
    "\n",
    "> __Note:__ marked as _experimental_\n",
    "\n",
    "## Correlation\n",
    "- MLlib Main Guide: https://spark.apache.org/docs/2.4.3/ml-statistics.html#correlation  \n",
    "- API Docs: https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.stat.Correlation  \n",
    "\n",
    "Calculating the correlation between two series of data is a common operation in Statistics. Spark MLlib provides the flexibility to calculate pairwise correlations among many series.\n",
    "\n",
    "### `ml.stat.Correlation`\n",
    "\n",
    "`Correlation` computes the correlation matrix for the input `DataFrame` of `Vector`s using the specified method. The output will be a `DataFrame` that contains the correlation matrix of the column of vectors.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Correlation_coefficient.png/800px-Correlation_coefficient.png\" alt=\"Correlation coefficient\" width=\"35%\"/>  \n",
    "<small>Examples of scatter diagrams with different values of correlation coefficient (ρ)<br/>Source: wikipedia</small>\n",
    "\n",
    "To use correlation, one uses the `Correlation` class from the `pyspark.ml.stat` module.  \n",
    "On this class, call the `.corr` method.\n",
    "\n",
    "Parameters for `Correlation.corr`\n",
    "- `dataset` – A DataFrame.\n",
    "- `column` – The name of the column of vectors for which the correlation coefficient needs to be computed. This must be a column of the dataset, and it must contain `Vector` objects.\n",
    "- `method` – String specifying the method to use for computing correlation. Supported: pearson (default), spearman.\n",
    "\n",
    "The supported correlation methods are currently:\n",
    "- [Pearson's correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) (default)  \n",
    "    `method=\"pearson\"`  \n",
    "    Code example:\n",
    "    ```python\n",
    "    Correlation.corr(dataframe, column=\"<column_of_vectors>\", method=\"pearson\")\n",
    "    ```\n",
    "\n",
    "\n",
    "- [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)  \n",
    "    Code example\n",
    "    ```python\n",
    "    df = DataFrame.cache()\n",
    "    Correlation.corr(df, column=\"<column_of_vectors>\", method=\"spearman\")\n",
    "    ```\n",
    "    \n",
    ">__Note:__ For Spearman, a rank correlation, Spark needs to create an `RDD[Double]` for each column and sort it in order to retrieve the ranks and then join the columns back into an `RDD[Vector]`, which is a fairly costly operation. Hence, cache the input `DataFrame` before calling `.corr` with `method=‘spearman’` to avoid recomputing the common lineage.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [\n",
    "    (Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "    (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "    (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "    (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# Pearson\n",
    "print(\"Pearson correlation matrix:\")\n",
    "r1 = Correlation.corr(df, \"features\")\n",
    "r1.show()\n",
    "print(f\"{r1.first()[0]}\\n\")\n",
    "\n",
    "# Spearman\n",
    "df = df.cache()\n",
    "print(\"Spearman correlation matrix:\")\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\")\n",
    "r2.show()\n",
    "print(f\"{r2.first()[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "- MLlib Main Guide: https://spark.apache.org/docs/2.4.3/ml-statistics.html#hypothesis-testing\n",
    "    \n",
    "Hypothesis testing is a powerful tool in statistics to determine whether a result is statistically significant, whether this result occurred by chance or not. Spark's MLlib Main Guide only makes mentions of the `ChiSquareTest`, but it also supports `KolmogorovSmirnovTest`. The `KolmogorovSmirnovTest` feature was newly added in Spark 2.4.0 and has not made it to the official guide yet.\n",
    "\n",
    "### `ml.stat.ChiSquareTest`\n",
    "- Wikipedia: [Chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test)  \n",
    "- API Docs: https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.stat.ChiSquareTest\n",
    "\n",
    "<p>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Chi-square_distributionCDF-English.png/600px-Chi-square_distributionCDF-English.png\" alt=\"Chi-square distribution\" width=\"30%\" /><br/>\n",
    "<small>Chi-squared distribution, showing χ2 on the x-axis and p-value (right tail probability) on the y-axis.<br/>Source: wikipedia</small>\n",
    "</p>\n",
    "\n",
    "Conducts Pearson’s independence test for every feature against the label. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed. All label and feature values must be categorical.\n",
    "\n",
    "The null hypothesis is that the occurrence of the outcomes is statistically independent.\n",
    "\n",
    "To use Chi-squared test, one uses the `ChiSquareTest` class from the `pyspark.ml.stat` module.  \n",
    "On this class, call the `.test` method.\n",
    "\n",
    "Parameters for `ChiSquareTest.test`\n",
    "\n",
    "- `dataset` – DataFrame of categorical labels and categorical features. Real-valued features will be treated as categorical for each distinct value.\n",
    "- `featuresCol` – Name of features column in dataset (must be of type `Vector`).\n",
    "- `labelCol` – Name of label column in dataset (any numerical type).\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "ChiSquareTest.test(dataframe, featuresCol=\"<features_column>\", labelCol=\"<label_column>\")\n",
    "```\n",
    "\n",
    "### `ml.stat.KolmogorovSmirnovTest`\n",
    "- Wikipedia: [Kolmogorov-Smirnov Test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)  \n",
    "- API Docs: https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.stat.KolmogorovSmirnovTest\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cf/KS_Example.png\" alt=\"Kolmogorov–Smirnov statistic\" width=\"30%\"/>  \n",
    "<small>Illustration of the Kolmogorov–Smirnov statistic. Red line is CDF, blue line is an ECDF, and the black arrow is the K–S statistic<br/>Source: wikipedia</small>\n",
    "\n",
    "Conduct the two-sided Kolmogorov Smirnov (KS) test for data sampled from a continuous distribution.\n",
    "\n",
    "By comparing the largest difference between the empirical cumulative distribution of the sample data and the theoretical distribution we can provide a test for the the null hypothesis that the sample data comes from that theoretical distribution.\n",
    "\n",
    "To use Kolmogorov–Smirnov, one uses the `KolmogorovSmirnovTest` class from the `pyspark.ml.stat` module.  \n",
    "On this class, call the `.test` method.\n",
    "\n",
    "Parameters for `KolmogorovSmirnovTest.test`\n",
    "(positional only)\n",
    "- `dataset` – a DataFrame containing the sample of data to test.\n",
    "- `sampleCol` – Name of sample column in dataset, of any numerical type.\n",
    "- `distName` – a string name for a theoretical distribution, (currently only support “norm”).\n",
    "- `*params` – Double values specifying the parameters to be used for the theoretical distribution. For “norm” distribution, the parameters includes mean and variance.\n",
    "    - `mean`\n",
    "    - `variance`\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "KolmogorovSmirnovTest.test(dataframe, \"<sample_column>\", \"norm\", 0.1, 1.0)\n",
    "```\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChiSquareTest:\n",
      "in:\n",
      "+-----+----------+\n",
      "|label|  features|\n",
      "+-----+----------+\n",
      "|  0.0|[0.5,10.0]|\n",
      "|  0.0|[1.5,20.0]|\n",
      "|  1.0|[1.5,30.0]|\n",
      "|  0.0|[3.5,30.0]|\n",
      "|  0.0|[3.5,40.0]|\n",
      "|  1.0|[3.5,40.0]|\n",
      "+-----+----------+\n",
      "\n",
      "out:\n",
      "+---------------------------------------+----------------+----------+\n",
      "|pValues                                |degreesOfFreedom|statistics|\n",
      "+---------------------------------------+----------------+----------+\n",
      "|[0.6872892787909721,0.6822703303362126]|[2, 3]          |[0.75,1.5]|\n",
      "+---------------------------------------+----------------+----------+\n",
      "\n",
      " pValues: [0.6872892787909721,0.6822703303362126]\n",
      " degreesOfFreedom: [2, 3]\n",
      " statistics: [0.75,1.5]\n",
      "\n",
      "KolmogorovSmirnovTest:\n",
      "in:\n",
      "+------+\n",
      "|sample|\n",
      "+------+\n",
      "|   0.1|\n",
      "|  0.15|\n",
      "|   0.2|\n",
      "|   0.3|\n",
      "|  0.25|\n",
      "+------+\n",
      "\n",
      "out:\n",
      "+-------------------+-----------------+\n",
      "|             pValue|        statistic|\n",
      "+-------------------+-----------------+\n",
      "|0.06821463111921133|0.539827837277029|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "DataFrame based result:\n",
      " pValue: 0.06821\n",
      " statistic: 0.53983\n",
      "\n",
      "RDD based result:\n",
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.539827837277029 \n",
      "pValue = 0.06821463111921133 \n",
      "Low presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest, KolmogorovSmirnovTest\n",
    "\n",
    "# ChiSquareTest\n",
    "data = [\n",
    "    (0.0, Vectors.dense(0.5, 10.0)),\n",
    "    (0.0, Vectors.dense(1.5, 20.0)),\n",
    "    (1.0, Vectors.dense(1.5, 30.0)),\n",
    "    (0.0, Vectors.dense(3.5, 30.0)),\n",
    "    (0.0, Vectors.dense(3.5, 40.0)),\n",
    "    (1.0, Vectors.dense(3.5, 40.0)),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\")\n",
    "print(\"\\nChiSquareTest:\")\n",
    "print(\"in:\")\n",
    "df.show()\n",
    "print(\"out:\")\n",
    "r.show(1, False)\n",
    "print(f\" pValues: {r.first().pValues}\")\n",
    "print(f\" degreesOfFreedom: {r.first().degreesOfFreedom}\")\n",
    "print(f\" statistics: {r.first().statistics}\")\n",
    "\n",
    "# KolmogorovSmirnovTest\n",
    "data = [[0.1], [0.15], [0.2], [0.3], [0.25]]\n",
    "df = spark.createDataFrame(data, [\"sample\"])\n",
    "\n",
    "r = KolmogorovSmirnovTest.test(df, 'sample', 'norm', 0.0, 1.0)\n",
    "print(\"\\nKolmogorovSmirnovTest:\")\n",
    "print(\"in:\")\n",
    "df.show()\n",
    "print(\"out:\")\n",
    "r.show()\n",
    "# Summary of the test including the p-value, test statistic\n",
    "print(\"DataFrame based result:\")\n",
    "print(f\" pValue: {round(r.first().pValue, 5)}\")\n",
    "print(f\" statistic: {round(r.first().statistic, 5)}\\n\")\n",
    "# Note that the Scala functionality of calling Statistics.kolmogorovSmirnovTest with\n",
    "# a lambda to calculate the CDF is not made available in the Python API\n",
    "\n",
    "# The RDD-based API for KolmogorovSmirnovTest outputs quite a bit nicer, which shows\n",
    "# the lack of feature parity with the RDD-based API for this feature\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# needs an rdd to run, creating an rdd with similar data\n",
    "data =  [0.1, 0.15, 0.2, 0.3, 0.25]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# run a KS test for the sample versus a standard normal distribution\n",
    "testResult = Statistics.kolmogorovSmirnovTest(rdd, \"norm\", 0, 1)\n",
    "# Note that the Scala functionality of calling Statistics.kolmogorovSmirnovTest with\n",
    "# a lambda to calculate the CDF is not made available in the Python API\n",
    "\n",
    "# summary of the test including the p-value, test statistic, and null hypothesis\n",
    "# if our p-value indicates significance, we can reject the null hypothesis\n",
    "print(\"RDD based result:\")\n",
    "print(testResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer\n",
    "\n",
    "Spark provides vector column summary statistics for Dataframes through Summarizer. Available metrics are the column-wise max, min, mean, variance, and number of nonzeros, as well as the total count. \n",
    "\n",
    "This feature was newly added in Spark 2.4.0 - hence it is quite 'fresh'. I found some typo's in the documentation related specifically to the function of this. Currently, the performance of this interface is about 2x-3x slower compared to using the equivalent RDD interface.\n",
    "\n",
    "\n",
    "### `ml.stat.Summarizer`\n",
    "API guide: https://spark.apache.org/docs/2.4.3/api/python/pyspark.ml.html#pyspark.ml.stat.Summarizer  \n",
    "Tools for vectorized statistics on MLlib Vectors. The methods in this package provide various statistics for `Vectors` contained inside `DataFrame`s. This class lets users pick the statistics they would like to extract for a given column.\n",
    "\n",
    "There are two ways to use this class, singular or multiple. Instructions below.\n",
    "\n",
    "#### 1. Computing singular metrics:\n",
    "- `mean(col, weightCol=None)`  \n",
    "coefficient-wise mean.\n",
    "- `variance(col, weightCol=None)`  \n",
    "coefficient-wise variance.\n",
    "- `count(col, weightCol=None)`  \n",
    "count of all vectors seen\n",
    "- `numNonZeros(col, weightCol=None)`  \n",
    "number of non-zeros for each coefficient\n",
    "- `max(col, weightCol=None)`  \n",
    "maximum for each coefficient\n",
    "- `min(col, weightCol=None)`  \n",
    "minimum for each coefficient\n",
    "- `normL1(col, weightCol=None)`  \n",
    "L1 norm of each coefficient (sum of the absolute values)\n",
    "- `normL2(col, weightCol=None)`  \n",
    "Euclidean norm for each coefficient\n",
    "    \n",
    "#### 2. Multiple metrics  \n",
    "To compute multiple metrics, first run `Summarizer.metrics` with the specific metrics that you want to compute.\n",
    "\n",
    "- `Summarizer.metrics(*metrics)`\n",
    "Given a list of metrics, provides a builder that computes metrics from a column.  \n",
    "Available metrics (same as singular metrics above): `mean`, `variance`, `count`, `numNonZeros`, `max`, `min`, `normL1`, `normL2`\n",
    "\n",
    "This returns an instance of the `SummaryBuilder` class. On this return instance of `SummaryBuilder` use the `.summary()` method.  \n",
    "```python\n",
    "SummaryBuilder.summary(\"featuresCol\", weightCol=None)\n",
    "```\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [\n",
    "    (1.0, Vectors.dense(1.0, 1.0, 1.0)),\n",
    "    (0.0, Vectors.dense(1.0, 2.0, 3.0)),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"weight\", \"features\"])\n",
    "\n",
    "# compute statistics for single metric \"mean\" with weight\n",
    "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# compute statistics for single metric \"mean\" without weight\n",
    "df.select(Summarizer.mean(df.features)).show(truncate=False)\n",
    "\n",
    "# create summarizer for multiple metrics \"mean\", \"count\", \"numNonZeros\"\n",
    "summarizer = Summarizer.metrics(\"mean\", \"count\", \"numNonZeros\")\n",
    "\n",
    "# compute statistics for multiple metrics with weight\n",
    "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# compute statistics for multiple metrics without weight\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.539827837277029 \n",
      "pValue = 0.06821463111921133 \n",
      "Low presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
