{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". You can run all the tests with the validate button. If the validate command takes too long, you can also confirm that you pass all the tests if you can run through the whole notebook without getting validation errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4f2a9a3135e9e1e2394550af72a2c36",
     "grade": false,
     "grade_id": "cell-7f2cae05492e6bb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e34ac3f62321e2ef28762e28cf1f2443",
     "grade": false,
     "grade_id": "cell-bab5d9941b66afa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Structured Streaming exercises\n",
    "\n",
    "In this problem set you will use structured streaming to analyze made-up trail camera data. We will simulate real-time streaming by having multiple data files and loading them one by one.\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe1fbfc4dee99cacf13c6425cebcdccf",
     "grade": false,
     "grade_id": "cell-45313625bf65eeee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: [SPARK_LOCAL_CONFIGS]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nghiaht7/.sdkman/candidates/spark/3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/13 14:12:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql\n",
    "from IPython.display import clear_output, display\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StructuredStreaming\").getOrCreate()\n",
    "\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/nghiaht7/data-engineer/big-data-platforms-f20.mooc.fi/StructuredStreaming/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f7edb5fd66706697afb2bc621de5b66",
     "grade": false,
     "grade_id": "cell-de5e235299290c20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Load data\n",
    "\n",
    "First we'll start with normal dataframe exercises. Create a method that loads the trail camera data into a dataframe. The data is in JSON format. You might have to specify the schema with the StructType methods. The dataframe will have null values called 'null', you can either remove them or leave them be. This dataframe simulates the input dataframe that we will use for streaming.\n",
    "\n",
    "param `path`: path to the JSON dataset.\n",
    "\n",
    "`return`: dataframe containing trail camera information.\n",
    "\n",
    "schema:\n",
    "\n",
    "Name | Type\n",
    "------| :-----\n",
    "time  | Timestamp (nullable = true)\n",
    "animal_name | String (nullable = true)\n",
    "weather | String (nullable = true)\n",
    "battery | Double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "881d43c4a14b7ce6557b05b0763e87aa",
     "grade": false,
     "grade_id": "cell-6b0ed104e544111a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame representing data in the JSON files\n",
    "def loadData(path):\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType(), True),\n",
    "            StructField(\"animal_name\", StringType(), True),\n",
    "            StructField(\"weather\", StringType(), True),\n",
    "            StructField(\"battery\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "    df = spark.read.schema(schema).json(path)\n",
    "    return df.na.drop(\"any\")\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------+-------+\n",
      "|               time|animal_name|weather|battery|\n",
      "+-------------------+-----------+-------+-------+\n",
      "|2020-04-18 21:50:40|       Deer|  Clear|    7.0|\n",
      "|2020-03-10 11:23:34|   Squirrel|  Clear|   86.0|\n",
      "|2019-11-09 20:36:04|   Squirrel|  Clear|   55.0|\n",
      "|2019-10-31 07:22:20|   Squirrel| Cloudy|   12.0|\n",
      "|2020-05-04 10:59:35|   Squirrel|  Clear|   64.0|\n",
      "|2020-01-30 14:21:35|   Squirrel|  Clear|   34.0|\n",
      "|2019-10-29 19:20:05|     Rabbit|  Rainy|    6.0|\n",
      "|2020-01-30 11:34:51|   Squirrel|  Rainy|   96.0|\n",
      "|2020-07-17 19:32:23|       Deer| Cloudy|   17.0|\n",
      "|2020-05-27 09:42:41|   Squirrel|  Rainy|    5.0|\n",
      "|2020-05-28 09:00:05|       Bear|  Storm|   53.0|\n",
      "|2020-08-07 11:02:38|       Deer| Cloudy|   14.0|\n",
      "|2019-06-22 16:02:52|     Rabbit|  Clear|   13.0|\n",
      "|2019-09-23 11:56:31|   Squirrel|  Clear|   95.0|\n",
      "|2020-05-31 07:27:03|   Squirrel|  Clear|  100.0|\n",
      "|2019-11-29 19:11:53|     Rabbit|  Clear|   71.0|\n",
      "|2020-03-23 20:06:24|     Rabbit|  Clear|   19.0|\n",
      "|2019-07-12 10:45:52|   Squirrel|  Rainy|   15.0|\n",
      "|2020-05-30 13:07:53|     Rabbit|  Rainy|   84.0|\n",
      "|2019-06-09 04:34:10|     Rabbit|  Clear|   95.0|\n",
      "+-------------------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "loadData(path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed7a7c3d8c4f4d13bb274dbea8e02d74",
     "grade": true,
     "grade_id": "cell-23cda96e1af452c0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"loadData tests\"\"\"\n",
    "\n",
    "cols = StructType(\n",
    "    [\n",
    "        StructField(\"time\", TimestampType(), True),\n",
    "        StructField(\"animal_name\", StringType(), True),\n",
    "        StructField(\"weather\", StringType(), True),\n",
    "        StructField(\"battery\", DoubleType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "testTs = datetime(2020, 1, 1)\n",
    "\n",
    "fakeData = [(testTs, \"dog\", \"cloudy\", 100.0)]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, cols)\n",
    "\n",
    "df = loadData(path)\n",
    "\n",
    "assert df.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (\n",
    "    fakeDf.dtypes,\n",
    "    df.dtypes,\n",
    ")\n",
    "\n",
    "test = str(loadData(path).sample(False, 0.01, seed=12345).limit(1).first())\n",
    "correct = \"Row(time=datetime.datetime(2020, 3, 10, 11, 23, 34), animal_name='Squirrel', weather='Clear', battery=86.0)\"\n",
    "assert test == correct, \"the row was expected to be %s but it was %s\" % (correct, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73db5636e96710521959e47c330f783e",
     "grade": false,
     "grade_id": "cell-5634898feaf9bf39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Animal count\n",
    "\n",
    "Next we will simulate the output dataframe that we will use for streaming. Create a method that counts the number of appearences for each animal. The dataframe should be sorted by count in descending order. You should remove the null rows now if you didn't do it in the last method.\n",
    "\n",
    "param `df`: trail camera dataframe created using `loadData`.\n",
    "\n",
    "`return`: dataframe containing number of appearences per animal. The dataframe should include columns \"animal_name\" and \"count\". \"count\"  should be in Long format, it should happen automatically with spark functions. The dataframe must not include count for null values.\n",
    "\n",
    "example output:\n",
    "\n",
    "animal_name|count\n",
    "-------:|-----\n",
    "Dog| 1234|\n",
    "Cat| 1111|\n",
    "Mouse| 999|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3549ed5221081c067054be0eb92d791",
     "grade": false,
     "grade_id": "cell-7200a4dc05503fec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def animalCount(df):\n",
    "    return (\n",
    "        df.groupBy(\"animal_name\")\n",
    "        .count()\n",
    "        .na.drop(\"any\")\n",
    "        .orderBy(\"count\", ascending=False)\n",
    "    )\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|animal_name|count|\n",
      "+-----------+-----+\n",
      "|     Rabbit|  801|\n",
      "|   Squirrel|  766|\n",
      "|       Deer|  238|\n",
      "|       Bear|   74|\n",
      "|       Wolf|   72|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# example print\n",
    "animalCount(loadData(path)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "animalCount(loadData(path)).sample(False, 0.1, seed=1).limit(1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "647a6791f8e9d3becc73fcaa1fce9a6b",
     "grade": true,
     "grade_id": "cell-2f1a372bdc6469a8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the row was expected to be Row(animal_name='Wolf', count=72) but it was None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1266809/2671631303.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimalCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mcorrect1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Row(animal_name='Wolf', count=72)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m assert test1 == correct1, \"the row was expected to be %s but it was %s\" % (\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mcorrect1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtest1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: the row was expected to be Row(animal_name='Wolf', count=72) but it was None"
     ]
    }
   ],
   "source": [
    "\"\"\"animalCount tests\"\"\"\n",
    "\n",
    "cols = StructType(\n",
    "    [\n",
    "        StructField(\"animal_name\", StringType(), True),\n",
    "        StructField(\"count\", LongType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "fakeData = [(\"dog\", 1)]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, cols)\n",
    "\n",
    "df = animalCount(loadData(path))\n",
    "\n",
    "assert df.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (\n",
    "    fakeDf.dtypes,\n",
    "    df.dtypes,\n",
    ")\n",
    "\n",
    "assert df.count() == 5, (\n",
    "    \"the number of rows was expected to be 5 but it was %s\" % df.count()\n",
    ")\n",
    "\n",
    "df = df.toPandas()\n",
    "\n",
    "assert (\n",
    "    df.loc[0][1] >= df.loc[1][1]\n",
    "), \"the first item was expected to have higher count than the second\"\n",
    "assert (\n",
    "    df.loc[3][1] >= df.loc[4][1]\n",
    "), \"the fourth item was expected to have higher count than the last\"\n",
    "assert df.loc[0][0] == \"Rabbit\", (\n",
    "    \"the first item was expected to be Rabbit but it was %s\" % df.loc[0][0]\n",
    ")\n",
    "assert df.loc[4][0] == \"Wolf\", (\n",
    "    \"the last item was expected to be Wolf but it was %s\" % df.loc[4][0]\n",
    ")\n",
    "\n",
    "test1 = str(animalCount(loadData(path)).sample(False, 0.1, seed=1).limit(1).first())\n",
    "correct1 = \"Row(animal_name='Wolf', count=72)\"\n",
    "assert test1 == correct1, \"the row was expected to be %s but it was %s\" % (\n",
    "    correct1,\n",
    "    test1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5f345af7e9f3a6f665df910a2172185",
     "grade": false,
     "grade_id": "cell-97647ea5af984e11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## inputDf\n",
    "\n",
    "Now we will finally do the streaming. First you should specify the schema for the input dataframe. The schema is the same as in the Load Data exercise. Then you should create the input dataframe with `spark.readStream` method. Remember to include the schema and the path. You will also have to include `.option(\"maxFilesPerTrigger\", 1)` so that we can simulate real-time streaming by loading one file at a time.\n",
    "\n",
    "param `path`: path to the JSON dataset.\n",
    "\n",
    "`return`: input dataframe containing trail camera information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b279959bef32628916d5754f784cf92",
     "grade": false,
     "grade_id": "cell-6856edef4018385b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def inputDf(path):\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType(), True),\n",
    "            StructField(\"animal_name\", StringType(), True),\n",
    "            StructField(\"weather\", StringType(), True),\n",
    "            StructField(\"battery\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    stream_spark_reader = spark.readStream.schema(schema).option(\n",
    "        \"maxFilesPerTrigger\", 1\n",
    "    )\n",
    "    df = stream_spark_reader.json(path).na.drop(\"any\")\n",
    "    return df\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddd2a8d6cebc27ff80ff38bff45eb159",
     "grade": false,
     "grade_id": "cell-527d914dbbb0585d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## outputDf\n",
    "\n",
    "Next you should create the output dataframe, similar to the Animal Count exercise. You will have to exclude the null values and sort the dataframe by count, descending order.\n",
    "\n",
    "param `inputDF`: input dataframe created by `inputDf()`.\n",
    "\n",
    "`return`: filtered and sorted dataframe containing the number of appearences per animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b290e9ac2650733cc7e0a85440760f06",
     "grade": false,
     "grade_id": "cell-7a2b6affcb2a634d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def outputDf(inputDF):\n",
    "    animal_count = (\n",
    "        inputDF.groupBy(\"animal_name\")\n",
    "        .count()\n",
    "        .na.drop(\"any\")\n",
    "        .orderBy(\"count\", ascending=False)\n",
    "    )\n",
    "    return animal_count\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5647344dda2b91bdd015760358f4990",
     "grade": false,
     "grade_id": "cell-48f8defd689bc680",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## createQuery\n",
    "\n",
    "Finally, you should start streaming the output dataframe with the `writeStream` method. You will have to include the options `format`=\"memory\", `queryName`=\"counts\" and `outputMode`=\"complete\".\n",
    "\n",
    "param `outputDF`: output dataframe created by `outputDf()`.\n",
    "\n",
    "`return`: a query on the output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bfac23b72e1e42d23635e0fadabc005",
     "grade": false,
     "grade_id": "cell-99580774911edd99",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def createQuery(outputDF):\n",
    "    stream_writer = (\n",
    "        outputDF.writeStream\n",
    "        # DataStream queries need to be named\n",
    "        .queryName(\"counts\")\n",
    "        .outputMode(\"complete\").format(\"memory\")\n",
    "    )\n",
    "    query = stream_writer.start()\n",
    "\n",
    "\n",
    "    \n",
    "    return query\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dde5484250857ae48737c4616d8b0c28",
     "grade": true,
     "grade_id": "cell-d6306f5d4ea1bb85",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/13 14:13:37 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-89b72bec-0568-444e-9a4d-8caf9132596d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"streaming tests\"\"\"\n",
    "inputStreamDf = inputDf(path)\n",
    "outputStreamDf = outputDf(inputStreamDf)\n",
    "query = createQuery(outputStreamDf)\n",
    "\n",
    "assert outputStreamDf.isStreaming, \"the outputDF was expected to be streaming\"\n",
    "\n",
    "df = spark.sql(\"select * from counts\")\n",
    "\n",
    "assert df.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (\n",
    "    fakeDf.dtypes,\n",
    "    df.dtypes,\n",
    ")\n",
    "\n",
    "status = {\n",
    "    \"message\": \"Processing new data\",\n",
    "    \"isDataAvailable\": True,\n",
    "    \"isTriggerActive\": True,\n",
    "}\n",
    "\n",
    "assert query.status == status, \"the status was expected to be %s but it was %s\" % (\n",
    "    status,\n",
    "    query.status,\n",
    ")\n",
    "\n",
    "x = df.count()\n",
    "assert df.count() == 0, (\n",
    "    \"the number of rows was expected to be 0 when the streaming just started but it was %s\"\n",
    "    % x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1266809/3556069979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputStreamDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputDf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutputStreamDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputDf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputStreamDf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputStreamDf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputDf' is not defined"
     ]
    }
   ],
   "source": [
    "inputStreamDf = inputDf(path)\n",
    "outputStreamDf = outputDf(inputStreamDf)\n",
    "query = createQuery(outputStreamDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputStreamDf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|animal_name|count|\n",
      "+-----------+-----+\n",
      "|     Rabbit|  204|\n",
      "|   Squirrel|  190|\n",
      "|       Deer|   58|\n",
      "|       Wolf|   18|\n",
      "|       Bear|   16|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * from counts\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|animal_name|count|\n",
      "+-----------+-----+\n",
      "|     Rabbit|  801|\n",
      "|   Squirrel|  766|\n",
      "|       Deer|  238|\n",
      "|       Bear|   74|\n",
      "|       Wolf|   72|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can print streaming here by adjusting n, but set n to 0 before submitting\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql(\"SELECT * FROM counts\").show())\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47116ae6f1cfe0c1e990a90669d80141",
     "grade": false,
     "grade_id": "cell-dd5b6197b17972e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
